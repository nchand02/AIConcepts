<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Chapter 3: Deep Learning - Neural networks, CNNs, RNNs, and modern architectures">
    <title>Chapter 3: Deep Learning | AI Tutorial</title>
    
    <link rel="stylesheet" href="../assets/css/style.css">
    <link rel="stylesheet" href="../assets/css/theme.css">
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
</head>
<body>
    <div class="site-wrapper">
        <header class="site-header">
            <div class="header-content">
                <div class="site-logo">
                    <h1><a href="../index.html">AI <span>Tutorial</span></a></h1>
                </div>
                <nav class="main-nav">
                    <ul>
                        <li><a href="../index.html">Home</a></li>
                        <li><a href="chapter1.html" class="active">Chapters</a></li>
                        <li><a href="chapter15.html">Glossary</a></li>
                        <li><button id="theme-toggle" class="theme-toggle">üåô</button></li>
                    </ul>
                </nav>
                <button id="mobile-menu-toggle" class="mobile-menu-toggle">‚ò∞</button>
            </div>
        </header>

        <div class="content-wrapper">
            <aside class="sidebar">
                <h3>Chapters</h3>
                <ul class="toc-list">
                    <li><a href="../index.html">üè† Home</a></li>
                    <li><a href="chapter1.html">Ch 1: AI Foundations</a></li>
                    <li><a href="chapter2.html">Ch 2: Machine Learning</a></li>
                    <li><a href="chapter3.html" class="active">Ch 3: Deep Learning</a></li>
                    <li><a href="chapter4.html">Ch 4: Transformers</a></li>
                    <li><a href="chapter5.html">Ch 5: LLMs</a></li>
                    <li><a href="chapter6.html">Ch 6: Generative AI</a></li>
                    <li><a href="chapter7.html">Ch 7: Embeddings</a></li>
                    <li><a href="chapter8.html">Ch 8: RAG</a></li>
                    <li><a href="chapter9.html">Ch 9: Fine-Tuning</a></li>
                    <li><a href="chapter10.html">Ch 10: Multimodal AI</a></li>
                    <li><a href="chapter11.html">Ch 11: Agents & MCP</a></li>
                    <li><a href="chapter12.html">Ch 12: Tooling</a></li>
                    <li><a href="chapter13.html">Ch 13: Infrastructure</a></li>
                    <li><a href="chapter14.html">Ch 14: System Design</a></li>
                    <li><a href="chapter15.html">Ch 15: Glossary</a></li>
                </ul>
            </aside>

            <main class="main-content">
                <h1>Chapter 3: Deep Learning</h1>
                
                <p class="chapter-intro">
                    Deep Learning has revolutionized AI by enabling machines to learn hierarchical representations 
                    from raw data. This chapter explores neural network architectures, backpropagation, convolutional 
                    and recurrent networks, and the techniques that power modern computer vision and sequence modeling.
                </p>

                <h2 id="introduction">From Perceptrons to Deep Networks</h2>
                
                <p>
                    Deep Learning refers to neural networks with multiple hidden layers that learn increasingly 
                    abstract representations of data. The "deep" in deep learning refers to the depth of the network‚Äî
                    the number of layers through which data is transformed.
                </p>

                <div class="diagram-container">
                    <div class="diagram-title">Neural Network Evolution</div>
                    <pre class="mermaid">
timeline
    title Evolution of Neural Networks
    1957 : Perceptron (Rosenblatt)
    1969 : Limitations Exposed (Minsky & Papert)
    1986 : Backpropagation (Rumelhart et al.)
    1998 : LeNet for Digit Recognition
    2006 : Deep Belief Networks (Hinton)
    2012 : AlexNet ImageNet Breakthrough
    2014-2017 : ResNet, VGG, Inception
    2017+ : Transformers Era
                    </pre>
                </div>

                <h2 id="neural-networks">Neural Network Fundamentals</h2>

                <h3>The Artificial Neuron</h3>
                <p>
                    An artificial neuron mimics biological neurons:
                </p>
                <pre><code>Output = Activation(Weighted_Sum + Bias)
y = f(w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ... + w‚Çôx‚Çô + b)
</code></pre>

                <h3>Components</h3>
                <ul>
                    <li><strong>Inputs (x):</strong> Feature values</li>
                    <li><strong>Weights (w):</strong> Learned parameters controlling input importance</li>
                    <li><strong>Bias (b):</strong> Learned offset parameter</li>
                    <li><strong>Activation Function (f):</strong> Introduces non-linearity</li>
                </ul>

                <h3>Common Activation Functions</h3>
                <ul>
                    <li><strong>ReLU (Rectified Linear Unit):</strong> f(x) = max(0, x) - Most popular, prevents vanishing gradients</li>
                    <li><strong>Sigmoid:</strong> f(x) = 1/(1 + e‚ÅªÀ£) - Outputs between 0 and 1</li>
                    <li><strong>Tanh:</strong> f(x) = (eÀ£ - e‚ÅªÀ£)/(eÀ£ + e‚ÅªÀ£) - Outputs between -1 and 1</li>
                    <li><strong>Softmax:</strong> Converts logits to probability distribution (classification)</li>
                    <li><strong>Leaky ReLU:</strong> f(x) = max(Œ±x, x) - Addresses dying ReLU problem</li>
                </ul>

                <h3>Network Architecture</h3>
                <div class="diagram-container">
                    <pre class="ascii-diagram">
Input Layer    Hidden Layers      Output Layer
   (x‚ÇÅ)          ‚óè‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚óè             (≈∑‚ÇÅ)
   (x‚ÇÇ)    ‚Üí     ‚óè‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚óè      ‚Üí      (≈∑‚ÇÇ)
   (x‚ÇÉ)          ‚óè‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚óè             (≈∑‚ÇÉ)
   (x‚Çô)

Fully Connected (Dense) Neural Network
                    </pre>
                </div>

                <h2 id="backpropagation">Backpropagation: How Networks Learn</h2>
                
                <p>
                    Backpropagation is the algorithm that enables deep learning by efficiently computing gradients 
                    of the loss function with respect to all network parameters.
                </p>

                <div class="deep-dive">
                    <div class="deep-dive-header">
                        <span>üî¨ Deep Dive: Backpropagation Algorithm</span>
                        <span class="deep-dive-icon">‚ñº</span>
                    </div>
                    <div class="deep-dive-content">
                        <div class="deep-dive-inner">
                            <h3>The Chain Rule in Action</h3>
                            <p>
                                Backpropagation applies the chain rule of calculus to compute how changes in each 
                                weight affect the final loss, working backward from output to input layers.
                            </p>
                            
                            <h4>Algorithm Steps</h4>
                            <ol>
                                <li><strong>Forward Pass:</strong> Compute activations layer by layer using current weights</li>
                                <li><strong>Loss Calculation:</strong> Compare predictions to true labels</li>
                                <li><strong>Backward Pass:</strong> Compute gradients layer by layer in reverse</li>
                                <li><strong>Weight Update:</strong> Adjust weights using gradient descent</li>
                            </ol>
                            
                            <h4>Gradient Computation</h4>
                            <pre><code>For each layer l:
‚àÇL/‚àÇw[l] = ‚àÇL/‚àÇa[l] √ó ‚àÇa[l]/‚àÇz[l] √ó ‚àÇz[l]/‚àÇw[l]

Where:
- L = loss function
- w[l] = weights at layer l
- a[l] = activations at layer l
- z[l] = pre-activation values
</code></pre>
                            
                            <h4>Vanishing Gradient Problem</h4>
                            <p>
                                In very deep networks with sigmoid/tanh activations, gradients can become extremely 
                                small, making early layers learn very slowly. Solutions:
                            </p>
                            <ul>
                                <li>Use ReLU activation functions</li>
                                <li>Batch normalization</li>
                                <li>Residual connections (skip connections)</li>
                                <li>Better weight initialization (Xavier, He initialization)</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <h2 id="cnns">Convolutional Neural Networks (CNNs)</h2>
                
                <p>
                    CNNs are specialized for processing grid-like data (images, video, time series) by exploiting 
                    spatial structure through convolution operations.
                </p>

                <h3>Key Components</h3>
                
                <h4>1. Convolutional Layers</h4>
                <ul>
                    <li>Apply learnable filters (kernels) to detect local patterns</li>
                    <li>Filters slide across input, computing dot products</li>
                    <li>Early layers detect edges, later layers detect complex patterns</li>
                    <li>Parameter sharing reduces model size</li>
                </ul>

                <h4>2. Pooling Layers</h4>
                <ul>
                    <li><strong>Max Pooling:</strong> Take maximum value in each region</li>
                    <li><strong>Average Pooling:</strong> Take average value in each region</li>
                    <li>Reduces spatial dimensions</li>
                    <li>Provides translation invariance</li>
                </ul>

                <h4>3. Fully Connected Layers</h4>
                <ul>
                    <li>Connect all neurons from previous layer</li>
                    <li>Typically used at the end for classification</li>
                </ul>

                <div class="diagram-container">
                    <div class="diagram-title">CNN Architecture</div>
                    <pre class="mermaid">
graph LR
    A[Input Image<br/>224√ó224√ó3] --> B[Conv Layer<br/>112√ó112√ó64]
    B --> C[Pooling<br/>56√ó56√ó64]
    C --> D[Conv Layer<br/>56√ó56√ó128]
    D --> E[Pooling<br/>28√ó28√ó128]
    E --> F[Flatten]
    F --> G[Dense Layer<br/>1000]
    G --> H[Softmax<br/>Classes]
    
    style A fill:#4d94ff,stroke:#333,stroke-width:2px,color:#fff
    style H fill:#28a745,stroke:#333,stroke-width:2px,color:#fff
                    </pre>
                </div>

                <h3>Famous CNN Architectures</h3>
                <ul>
                    <li><strong>LeNet (1998):</strong> First CNN for digit recognition</li>
                    <li><strong>AlexNet (2012):</strong> ImageNet breakthrough, 8 layers</li>
                    <li><strong>VGG (2014):</strong> Deep networks with small filters</li>
                    <li><strong>GoogLeNet/Inception (2014):</strong> Multiple filter sizes in parallel</li>
                    <li><strong>ResNet (2015):</strong> Skip connections enable very deep networks (152+ layers)</li>
                    <li><strong>EfficientNet (2019):</strong> Compound scaling for efficiency</li>
                </ul>

                <h3>Applications</h3>
                <ul>
                    <li>Image classification and object detection</li>
                    <li>Facial recognition and verification</li>
                    <li>Medical image analysis</li>
                    <li>Autonomous vehicle perception</li>
                    <li>Video analysis and action recognition</li>
                </ul>

                <h2 id="rnns">Recurrent Neural Networks (RNNs)</h2>
                
                <p>
                    RNNs process sequential data by maintaining hidden state that captures information about 
                    previous inputs, enabling them to model temporal dependencies.
                </p>

                <h3>How RNNs Work</h3>
                <pre><code>h_t = tanh(W_hh √ó h_{t-1} + W_xh √ó x_t + b_h)
y_t = W_hy √ó h_t + b_y

Where:
- h_t = hidden state at time t
- x_t = input at time t
- y_t = output at time t
- W = weight matrices
- b = bias vectors
</code></pre>

                <div class="diagram-container">
                    <pre class="ascii-diagram">
Unrolled RNN Through Time:

h‚ÇÄ ‚Üí h‚ÇÅ ‚Üí h‚ÇÇ ‚Üí h‚ÇÉ ‚Üí h‚ÇÑ
     ‚Üë    ‚Üë    ‚Üë    ‚Üë
     x‚ÇÅ   x‚ÇÇ   x‚ÇÉ   x‚ÇÑ
     ‚Üì    ‚Üì    ‚Üì    ‚Üì
     y‚ÇÅ   y‚ÇÇ   y‚ÇÉ   y‚ÇÑ
                    </pre>
                </div>

                <h3>RNN Variants</h3>

                <h4>LSTM (Long Short-Term Memory)</h4>
                <p>
                    LSTMs address the vanishing gradient problem in standard RNNs through gating mechanisms:
                </p>
                <ul>
                    <li><strong>Forget Gate:</strong> Decides what to discard from cell state</li>
                    <li><strong>Input Gate:</strong> Decides what new information to store</li>
                    <li><strong>Output Gate:</strong> Decides what to output based on cell state</li>
                    <li><strong>Cell State:</strong> Long-term memory that flows through network</li>
                </ul>

                <h4>GRU (Gated Recurrent Unit)</h4>
                <p>
                    Simplified LSTM variant with fewer parameters:
                </p>
                <ul>
                    <li><strong>Update Gate:</strong> Controls how much past information to keep</li>
                    <li><strong>Reset Gate:</strong> Controls how much past information to forget</li>
                    <li>Faster training than LSTM with similar performance</li>
                </ul>

                <h3>Applications</h3>
                <ul>
                    <li>Natural language processing (translation, generation)</li>
                    <li>Speech recognition and synthesis</li>
                    <li>Time series prediction</li>
                    <li>Music generation</li>
                    <li>Video captioning</li>
                </ul>

                <div class="callout callout-note">
                    <div class="callout-title">üìù Note: The Transformer Revolution</div>
                    <p>
                        While RNNs dominated sequence modeling for years, Transformers (Chapter 4) have largely 
                        replaced them in NLP due to better parallelization and ability to capture long-range 
                        dependencies through attention mechanisms.
                    </p>
                </div>

                <h2 id="training">Training Deep Networks</h2>

                <h3>Optimization Techniques</h3>
                <ul>
                    <li><strong>Mini-Batch Gradient Descent:</strong> Standard approach, balances speed and accuracy</li>
                    <li><strong>Momentum:</strong> Accelerates learning in consistent directions</li>
                    <li><strong>Adam (Adaptive Moment Estimation):</strong> Most popular optimizer, adapts learning rates</li>
                    <li><strong>Learning Rate Scheduling:</strong> Reduce learning rate over time</li>
                </ul>

                <h3>Regularization Techniques</h3>
                <ul>
                    <li><strong>Dropout:</strong> Randomly deactivate neurons during training</li>
                    <li><strong>Batch Normalization:</strong> Normalize activations within mini-batches</li>
                    <li><strong>Data Augmentation:</strong> Create training variations (flips, crops, rotations)</li>
                    <li><strong>L1/L2 Regularization:</strong> Penalize large weights</li>
                    <li><strong>Early Stopping:</strong> Stop training when validation performance plateaus</li>
                </ul>

                <h3>Transfer Learning</h3>
                <p>
                    Use pre-trained models on large datasets, then fine-tune for specific tasks:
                </p>
                <ol>
                    <li>Start with model trained on large dataset (e.g., ImageNet)</li>
                    <li>Remove final classification layer</li>
                    <li>Add new layers for your specific task</li>
                    <li>Fine-tune with your data</li>
                </ol>

                <div class="callout callout-success">
                    <div class="callout-title">‚úÖ Transfer Learning Benefits</div>
                    <ul>
                        <li>Requires much less data</li>
                        <li>Faster training</li>
                        <li>Often achieves better performance</li>
                        <li>Leverages general features learned from large datasets</li>
                    </ul>
                </div>

                <h2 id="challenges">Challenges and Best Practices</h2>

                <h3>Common Challenges</h3>
                <ul>
                    <li><strong>Computational Cost:</strong> Training deep networks requires significant GPU resources</li>
                    <li><strong>Data Requirements:</strong> Deep learning typically needs large labeled datasets</li>
                    <li><strong>Hyperparameter Tuning:</strong> Many parameters to configure (architecture, learning rate, etc.)</li>
                    <li><strong>Interpretability:</strong> Difficult to understand why networks make specific decisions</li>
                    <li><strong>Adversarial Examples:</strong> Small input perturbations can fool networks</li>
                </ul>

                <h3>Best Practices</h3>
                <ul>
                    <li>Start with established architectures and pre-trained models</li>
                    <li>Use appropriate data preprocessing and augmentation</li>
                    <li>Monitor training and validation curves to detect overfitting</li>
                    <li>Use techniques like batch normalization and dropout</li>
                    <li>Experiment with different optimizers and learning rates</li>
                    <li>Leverage transfer learning when possible</li>
                    <li>Use mixed precision training for faster computation</li>
                </ul>

                <div class="key-terms">
                    <h3>Key Terms</h3>
                    <dl>
                        <dt>Deep Learning</dt>
                        <dd>Machine learning using neural networks with multiple hidden layers</dd>
                        
                        <dt>Backpropagation</dt>
                        <dd>Algorithm for computing gradients in neural networks using the chain rule</dd>
                        
                        <dt>Convolutional Neural Network (CNN)</dt>
                        <dd>Neural network specialized for processing grid-like data using convolution operations</dd>
                        
                        <dt>Recurrent Neural Network (RNN)</dt>
                        <dd>Neural network that processes sequences by maintaining hidden state</dd>
                        
                        <dt>LSTM</dt>
                        <dd>Long Short-Term Memory; RNN variant that handles long-range dependencies</dd>
                        
                        <dt>Activation Function</dt>
                        <dd>Non-linear function applied to neuron outputs (ReLU, sigmoid, tanh)</dd>
                        
                        <dt>Transfer Learning</dt>
                        <dd>Reusing pre-trained model weights for related tasks</dd>
                        
                        <dt>Dropout</dt>
                        <dd>Regularization technique that randomly deactivates neurons during training</dd>
                    </dl>
                </div>

                <div class="review-questions">
                    <h3>Review Questions</h3>
                    <ol>
                        <li>How does backpropagation enable neural networks to learn?</li>
                        <li>What advantages do CNNs have for image processing?</li>
                        <li>Explain the vanishing gradient problem and solutions.</li>
                        <li>How do LSTMs address limitations of standard RNNs?</li>
                        <li>What is transfer learning and when is it beneficial?</li>
                        <li>Compare and contrast CNNs and RNNs in terms of architecture and applications.</li>
                        <li>Why is ReLU the most popular activation function?</li>
                        <li>What regularization techniques help prevent overfitting in deep networks?</li>
                    </ol>
                </div>

                <div class="exercises">
                    <h3>Practical Exercises</h3>
                    <ol>
                        <li>
                            <strong>CNN Implementation:</strong> Build a CNN for image classification using a framework 
                            (TensorFlow/PyTorch). Experiment with different architectures and visualize learned filters.
                        </li>
                        <li>
                            <strong>Transfer Learning:</strong> Use a pre-trained model (ResNet, VGG) and fine-tune it 
                            on a small custom dataset. Compare performance with training from scratch.
                        </li>
                        <li>
                            <strong>RNN for Text:</strong> Implement a character-level RNN or LSTM for text generation. 
                            Train on a corpus and analyze generated samples.
                        </li>
                        <li>
                            <strong>Hyperparameter Study:</strong> Systematically vary learning rate, batch size, and 
                            network depth. Plot learning curves and analyze impact on convergence.
                        </li>
                    </ol>
                </div>

                <div class="chapter-navigation">
                    <a href="chapter2.html" class="nav-button prev">‚Üê Chapter 2: Machine Learning</a>
                    <a href="chapter4.html" class="nav-button next">Next: Transformers ‚Üí</a>
                </div>

            </main>
        </div>

        <footer class="site-footer">
            <p>&copy; 2025 AI Tutorial For Everyone</p>
        </footer>
    </div>

    <div id="ai-chat-widget-placeholder"></div>
    
    <script src="../assets/js/darkmode.js"></script>
    <script src="../assets/js/nav.js"></script>
    <script src="../assets/js/main.js"></script>
</body>
</html>
