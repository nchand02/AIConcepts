---
import BaseLayout from '../../layouts/BaseLayout.astro';
---

<BaseLayout 
  title="Chapter 5: Large Language Models" 
  description="Comprehensive guide to Large Language Models, their architecture, training, and capabilities"
  currentPage="/chapters/05-llms"
>
  <article>
    <h1>Chapter 5: Large Language Models (LLMs)</h1>
    
    <div class="callout info">
      <div class="callout-title">ğŸ“š Chapter Overview</div>
      <p>Large Language Models represent the culmination of Transformer technology at massive scale. This chapter explores how models like GPT-4, Claude, and Llama achieve remarkable capabilities through scale, training techniques, and emergent abilities.</p>
    </div>

    <div class="callout info">
      <div class="callout-title">ğŸŒŸ The LLM Revolution (2020-2025)</div>
      <p>The release of <strong>GPT-3 in May 2020</strong> (175B parameters, trained on 300B tokens, $4.6M estimated training cost) marked a watershed moment, demonstrating that massive scale enables <em>few-shot learning</em>â€”adapting to new tasks with just 3-5 examples in the prompt, no gradient updates required. This capability appeared suddenly around 13B parameters (GPT-2 1.5B couldn't, GPT-3 175B could), exemplifying <strong>emergent abilities</strong> that arise unpredictably at sufficient scale.</p>
      <p><strong>ChatGPT</strong> (November 2022) brought LLMs to mainstream consciousness with 100M users in 2 months, the fastest-growing consumer application in history. Built from GPT-3.5 fine-tuned with <em>Reinforcement Learning from Human Feedback (RLHF)</em>, it demonstrated how alignment techniques make models helpful, harmless, and honest. RLHF (introduced by Christiano et al. 2017, refined by OpenAI in InstructGPT 2022) uses human preference rankings to train a reward model, then optimizes the LLM via Proximal Policy Optimization (PPO) to maximize this reward while staying close to the supervised fine-tuned baseline.</p>
      <p>By 2023-2025, the field exploded: <strong>GPT-4</strong> (March 2023, estimated ~1.7T parameters mixture of experts, multimodal text+images, 32K-128K context, 90th percentile bar exam, 86% MMLU benchmark), <strong>Claude 3</strong> (March 2024, 200K context window, constitutional AI self-critiques using ethical principles), <strong>Llama 3</strong> (April 2024, 70B/405B open-weight models trained on 15T tokens, democratizing access with Apache 2.0 license), <strong>Gemini 1.5 Pro</strong> (February 2024, 1 million token context processing entire codebases/books), and reasoning models like <strong>OpenAI o1</strong> (September 2024, generates internal chain-of-thought before answering, 83% on International Math Olympiad qualifying vs. 13% for GPT-4o). The <strong>Chinchilla scaling laws</strong> (Hoffmann et al., DeepMind, March 2022) revealed most models were undertrainedâ€”optimal training requires D_opt â‰ˆ 20Ã—N parameters worth of tokens (e.g., 70B params needs 1.4T tokens, not 300B)â€”reshaping research toward compute-efficient training over pure parameter scaling.</p>
      <p><a href="https://en.wikipedia.org/wiki/Large_language_model" target="_blank">Wikipedia: Large language models</a> | <a href="https://en.wikipedia.org/wiki/GPT-3" target="_blank">Wikipedia: GPT-3</a> | <a href="https://en.wikipedia.org/wiki/ChatGPT" target="_blank">Wikipedia: ChatGPT</a> | <a href="https://en.wikipedia.org/wiki/Chinchilla_AI" target="_blank">Wikipedia: Chinchilla scaling</a></p>
    </div>

    <h2>What are Large Language Models?</h2>
    
    <p>LLMs are Transformer-based neural networks trained on massive text corpora to predict the next token in a sequence. Key characteristics:</p>

    <ul>
      <li><strong>Scale:</strong> Billions to trillions of parameters</li>
      <li><strong>Pre-training:</strong> Unsupervised learning on huge datasets (web-scale text)</li>
      <li><strong>Few-shot learning:</strong> Adapt to new tasks with minimal examples</li>
      <li><strong>Emergent abilities:</strong> Capabilities that appear only at scale</li>
      <li><strong>General purpose:</strong> Handle diverse tasks without task-specific training</li>
    </ul>

    <h2>Evolution of LLMs</h2>

    <table>
      <thead>
        <tr>
          <th>Model</th>
          <th>Year</th>
          <th>Parameters</th>
          <th>Key Innovation</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><strong>GPT-1</strong></td>
          <td>2018</td>
          <td>117M</td>
          <td>Unsupervised pre-training + supervised fine-tuning</td>
        </tr>
        <tr>
          <td><strong>BERT</strong></td>
          <td>2018</td>
          <td>340M</td>
          <td>Bidirectional pre-training, masked language modeling</td>
        </tr>
        <tr>
          <td><strong>GPT-2</strong></td>
          <td>2019</td>
          <td>1.5B</td>
          <td>Zero-shot learning, coherent long-form generation</td>
        </tr>
        <tr>
          <td><strong>T5</strong></td>
          <td>2019</td>
          <td>11B</td>
          <td>Text-to-text framework, unified approach</td>
        </tr>
        <tr>
          <td><strong>GPT-3</strong></td>
          <td>2020</td>
          <td>175B</td>
          <td>Few-shot in-context learning</td>
        </tr>
        <tr>
          <td><strong>Chinchilla</strong></td>
          <td>2022</td>
          <td>70B</td>
          <td>Optimal compute-efficient scaling laws</td>
        </tr>
        <tr>
          <td><strong>PaLM</strong></td>
          <td>2022</td>
          <td>540B</td>
          <td>Breakthrough on reasoning tasks</td>
        </tr>
        <tr>
          <td><strong>GPT-4</strong></td>
          <td>2023</td>
          <td>~1.7T</td>
          <td>Multimodal, advanced reasoning, longer context</td>
        </tr>
        <tr>
          <td><strong>Claude 3</strong></td>
          <td>2024</td>
          <td>Unknown</td>
          <td>200K context, constitutional AI</td>
        </tr>
        <tr>
          <td><strong>Llama 3</strong></td>
          <td>2024</td>
          <td>70B/405B</td>
          <td>Open weights, excellent performance</td>
        </tr>
      </tbody>
    </table>

    <h2>LLM Architecture</h2>

    <h3>Decoder-Only Transformers</h3>
    
    <p>Most modern LLMs (GPT, Llama, Claude) use decoder-only architecture:</p>

    <pre set:html={`<code>Architecture Stack (e.g., GPT-4):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Token + Position Embeddings     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Transformer Layer 1              â”‚
â”‚   - Multi-Head Attention         â”‚
â”‚   - Feed-Forward Network         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Transformer Layer 2              â”‚
â”‚   ...                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Transformer Layer N (96+ layers) â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Layer Normalization              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Output Projection (vocab_size)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Typical specs (GPT-3 175B):
- Layers: 96
- Hidden size: 12,288
- Attention heads: 96
- Context window: 2,048 â†’ 128K (GPT-4 Turbo)</code>`} />

    <h3>Key Architectural Innovations</h3>

    <ul>
      <li><strong>Grouped Query Attention (GQA):</strong> Reduces memory, maintains quality</li>
      <li><strong>RoPE (Rotary Position Embeddings):</strong> Better position encoding for long context</li>
      <li><strong>SwiGLU Activation:</strong> Replaces GELU for better performance</li>
      <li><strong>RMSNorm:</strong> Simpler, faster than LayerNorm</li>
      <li><strong>Mixture of Experts:</strong> Conditional computation for massive scale</li>
    </ul>

    <h2>Training Process</h2>

    <h3>Phase 1: Pre-training</h3>
    
    <p>Unsupervised learning on massive text corpora:</p>

    <pre set:html={`<code>Objective: Next Token Prediction
Given: "The cat sat on the"
Predict: "mat" (or any valid continuation)

Loss = -log P(w_t | w_1, ..., w_{{t-1}})

Training Data Scale:
- GPT-3: ~500B tokens
- Llama 2: 2T tokens  
- GPT-4: Estimated 10T+ tokens

Training Compute:
- GPT-3: ~3,640 petaflop-days
- PaLM: ~2,500 petaflop-days
- Modern LLMs: 10,000+ petaflop-days

Cost: $2M - $100M+ per training run</code>`} />

    <div class="callout info">
      <div class="callout-title">ğŸ“Š GPT-3 Training: Scale and Composition (May 2020)</div>
      <p><strong>Dataset composition</strong> (300B tokens, 570GB total, weighted sampling with fuzzy deduplication via Apache Spark MinHash LSH):</p>
      <ul>
        <li><strong>CommonCrawl (filtered):</strong> 410B byte-pair-encoded tokens, 60% of weighted training mix, 2016-2019 web scrapes across 60M domains</li>
        <li><strong>WebText2:</strong> 19B tokens, 22% weight, Reddit outbound links with â‰¥3 karma (quality filter)</li>
        <li><strong>Books1:</strong> 12B tokens, 8% weight, diverse book corpus</li>
        <li><strong>Books2:</strong> 55B tokens, 8% weight, additional book sources</li>
        <li><strong>English Wikipedia:</strong> 3B tokens, 3% weight, high-quality factual text</li>
      </ul>
      <p><strong>Training infrastructure:</strong> Hypothetical single GPU training would require ~355 years and cost $4.6M (Lambdalabs 2020 estimate). Actual training used thousands of GPUs in parallel, completing in weeks. Model uses 175B parameters with 16-bit precision (float16), requiring 350GB storage (2 bytes Ã— 175B). Context window: 2,048 tokens (~1,500 words), limiting long-document understanding.</p>
      <p><strong>Key finding:</strong> GPT-3 achieved 52% accuracy on human vs. AI text detection (barely better than random 50%), indicating coherent human-like generation. Enabled few-shot learning across diverse tasks: translation, question answering, arithmetic, code generation (CSS, JSX, Python), article writing (The Guardian used it to write an essay about AI being harmless).</p>
      <p><strong>Microsoft exclusive license</strong> (September 2020): Microsoft gained exclusive access to GPT-3's source code for embedding/modification, while OpenAI continues offering public API for text input/output without model download.</p>
      <p><a href="https://en.wikipedia.org/wiki/GPT-3#Training_and_capabilities" target="_blank">Wikipedia: GPT-3 training details</a></p>
    </div>

    <h3>Phase 2: Supervised Fine-Tuning (SFT)</h3>
    
    <p>Train on high-quality instruction-response pairs:</p>

    <pre set:html={`<code>Example training data:
{{
  "instruction": "Explain quantum computing to a 10-year-old",
  "response": "Imagine a magical coin that can be heads AND tails at the same time..."
}}

Dataset size: 10K - 1M examples
Duration: Hours to days
Purpose: Align model to follow instructions</code>`} />

    <h3>Phase 3: Reinforcement Learning from Human Feedback (RLHF)</h3>
    
    <p>Optimize for human preferences:</p>

    <div class="diagram-container">
      <pre class="mermaid" set:html={`graph TB
    A[Generate Multiple Responses] --> B[Human Ranks Responses]
    B --> C[Train Reward Model]
    C --> D[PPO Training]
    D --> E[Improved LLM]
    E --> A
    
    style A fill:#e1f5ff
    style E fill:#d4edda
      `} />
    </div>

    <pre set:html={`<code>Steps:
1. Collect human rankings of model outputs
2. Train reward model to predict human preferences
3. Use PPO (Proximal Policy Optimization) to optimize LLM
4. Balance: maximize reward while staying close to SFT model

Key challenge: Reward hacking (model exploits reward function)</code>`} />

    <div class="callout info">
      <div class="callout-title">ğŸ“ RLHF: From Theory to ChatGPT</div>
      <p><strong>Origins:</strong> Introduced by Christiano et al. (OpenAI, June 2017, "Deep Reinforcement Learning from Human Preferences") for Atari games and robotics. Adapted to language by Ouyang et al. (OpenAI, January 2022, "Training language models to follow instructions with human feedback") for InstructGPT, then refined into ChatGPT (November 2022).</p>
      <p><strong>Detailed pipeline:</strong></p>
      <ol>
        <li><strong>Supervised Fine-Tuning (SFT):</strong> Train on ~13K high-quality human-written prompt-response pairs to establish helpful baseline behavior.</li>
        <li><strong>Reward Model Training:</strong> Collect ~33K human preference comparisons (rank 4-9 model outputs per prompt). Train reward model (6B parameters, smaller than policy) to predict scalar reward matching human rankings. Accuracy: ~73% agreement with held-out human preferences.</li>
        <li><strong>PPO Optimization:</strong> Use Proximal Policy Optimization (Schulman et al. 2017) to maximize reward model score while constraining KL divergence from SFT model (prevents mode collapse and reward hacking). Batch size ~512 prompts, 256 PPO epochs per batch. Key hyperparameter: KL penalty coefficient Î² balances reward vs. staying on-distribution.</li>
      </ol>
      <p><strong>Constitutional AI</strong> (Anthropic, Dec 2022): Alternative approach where AI assistant critiques and revises its own responses against ethical principles ("constitution"), reducing reliance on human feedback. Used in Claude models with 200K context windows.</p>
      <p><strong>Modern variants:</strong> <em>Direct Preference Optimization (DPO)</em> (Rafailov et al., May 2023) bypasses explicit reward model, optimizing policy directly from preferences. <em>Reinforcement Learning from AI Feedback (RLAIF)</em> uses LLMs instead of humans to generate preference labels, scaling feedback collection.</p>
      <p><a href="https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback" target="_blank">Wikipedia: RLHF</a></p>
    </div>

    <h2>Scaling Laws</h2>

    <h3>Chinchilla Scaling Laws</h3>
    
    <p>Optimal model size vs. training data:</p>

    <pre set:html={`<code>Key Finding: Most LLMs are undertrained!

For compute budget C:
- Optimal model parameters: N_opt âˆ C^0.5
- Optimal training tokens: D_opt âˆ C^0.5

Example:
- Previous approach: 175B params, 300B tokens (GPT-3)
- Compute-optimal: 70B params, 1.4T tokens (Chinchilla)
- Result: Chinchilla outperforms GPT-3 despite fewer parameters

Implication: Training data is as important as model size</code>`} />

    <h3>Emergent Abilities</h3>
    
    <p>Capabilities that suddenly appear at sufficient scale:</p>

    <ul>
      <li><strong>Few-shot learning:</strong> Learn new tasks from examples in prompt</li>
      <li><strong>Chain-of-thought reasoning:</strong> Break down complex problems step-by-step</li>
      <li><strong>In-context learning:</strong> Adapt behavior based on prompt context</li>
      <li><strong>Multi-step reasoning:</strong> Solve problems requiring multiple inferential steps</li>
      <li><strong>Instruction following:</strong> Understand and execute complex instructions</li>
    </ul>

    <div class="callout info">
      <div class="callout-title">ğŸ¯ Emergent Abilities: Discontinuous Phase Transitions</div>
      <p><strong>Definition:</strong> Abilities that are not present in smaller models but appear suddenly at a specific scale, creating a "break" in the scaling curve where performance jumps discontinuously (Wei et al. 2022, "Emergent Abilities of Large Language Models"). These transitions appear on log-linear plots as sharp kinks rather than smooth extrapolations.</p>
      <p><strong>Examples with thresholds:</strong></p>
      <ul>
        <li><strong>Few-shot in-context learning</strong> (GPT-3, May 2020): Emerged around 13B parameters. GPT-2 (1.5B) couldn't adapt from prompt examples, GPT-3 (175B) could perform arithmetic, phonetic alphabet decoding, word unscrambling, and spatial reasoning purely from context.</li>
        <li><strong>Chain-of-thought prompting</strong> (Wei et al., Jan 2022): Only improved performance for models â‰¥62B parameters. Smaller models performed better with direct answers. Enables breaking down math word problems: "Roger has 5 tennis balls..." â†’ step-by-step arithmetic â†’ "Answer: 11 balls."</li>
        <li><strong>Advanced reasoning</strong>: Multi-hop reasoning (connecting facts across sentences), arithmetic beyond memorization, logical deduction, and compositional generalization all show discontinuous emergence at different scales.</li>
        <li><strong>Multilingual tasks</strong>: Ability to translate between languages never explicitly paired in training data, generating Kiswahili proverb equivalents in English, or detecting offensive content in code-mixed Hinglish.</li>
      </ul>
      <p><strong>Debate:</strong> Schaeffer et al. (2023) argue emergent abilities may be measurement artifactsâ€”when metrics use step functions (e.g., exact match accuracy) rather than continuous measures (e.g., token probability), smooth improvements appear as sudden jumps. However, true qualitative shifts in capability (like compositional reasoning) remain compelling evidence for emergence.</p>
      <p><a href="https://en.wikipedia.org/wiki/Large_language_model#Emergent_abilities" target="_blank">Wikipedia: Emergent abilities</a></p>
    </div>

    <h2>Prompting Techniques</h2>

    <h3>Zero-Shot Prompting</h3>
    
    <pre set:html={`<code>Prompt: "Translate to French: Hello, how are you?"
Output: "Bonjour, comment allez-vous?"</code>`} />

    <h3>Few-Shot Prompting</h3>
    
    <pre set:html={`<code>Prompt:
"Translate to French:
English: Hello â†’ French: Bonjour
English: Goodbye â†’ French: Au revoir  
English: Thank you â†’ French: Merci
English: Good morning â†’ French:"

Output: "Bonjour" or "Bon matin"</code>`} />

    <h3>Chain-of-Thought (CoT)</h3>
    
    <pre set:html={`<code>Prompt: "Roger has 5 tennis balls. He buys 2 more cans of tennis balls. 
Each can has 3 tennis balls. How many tennis balls does he have now?

Let's think step by step:"

Output:
"1. Roger starts with 5 tennis balls
2. He buys 2 cans
3. Each can has 3 balls, so 2 Ã— 3 = 6 balls
4. Total: 5 + 6 = 11 tennis balls
Answer: 11"</code>`} />

    <h3>Advanced Prompting Patterns</h3>

    <ul>
      <li><strong>ReAct:</strong> Reasoning + Acting (interleave thought and action)</li>
      <li><strong>Self-Consistency:</strong> Generate multiple answers, choose most common</li>
      <li><strong>Tree of Thoughts:</strong> Explore multiple reasoning paths</li>
      <li><strong>Automatic Prompt Engineering:</strong> Let LLMs optimize prompts</li>
    </ul>

    <h2>Context Window</h2>

    <table>
      <thead>
        <tr>
          <th>Model</th>
          <th>Context Length</th>
          <th>Notes</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>GPT-3</td>
          <td>2,048 tokens</td>
          <td>~1,500 words</td>
        </tr>
        <tr>
          <td>GPT-3.5</td>
          <td>4,096 tokens</td>
          <td>~3,000 words</td>
        </tr>
        <tr>
          <td>GPT-4</td>
          <td>8,192 / 32,768 tokens</td>
          <td>~6K / 24K words</td>
        </tr>
        <tr>
          <td>GPT-4 Turbo</td>
          <td>128,000 tokens</td>
          <td>~96,000 words (~300 pages)</td>
        </tr>
        <tr>
          <td>Claude 3</td>
          <td>200,000 tokens</td>
          <td>~150,000 words (~500 pages)</td>
        </tr>
        <tr>
          <td>Gemini 1.5 Pro</td>
          <td>1,000,000 tokens</td>
          <td>~750,000 words</td>
        </tr>
      </tbody>
    </table>

    <h2>Limitations and Challenges</h2>

    <div class="callout warning">
      <div class="callout-title">âš ï¸ Known Issues</div>
      <ul>
        <li><strong>Hallucinations:</strong> Generate plausible but false information</li>
        <li><strong>Lack of grounding:</strong> No connection to real-world facts</li>
        <li><strong>Reasoning limitations:</strong> Struggle with complex logic and math</li>
        <li><strong>Outdated knowledge:</strong> Training data has cutoff date</li>
        <li><strong>Inconsistency:</strong> Different responses to same question</li>
        <li><strong>Bias:</strong> Reflect biases in training data</li>
        <li><strong>No true understanding:</strong> Pattern matching, not comprehension</li>
      </ul>
    </div>

    <h2>Best Practices</h2>

    <div class="callout success">
      <div class="callout-title">âœ… Effective LLM Usage</div>
      <ul>
        <li><strong>Clear instructions:</strong> Be specific and unambiguous</li>
        <li><strong>Provide context:</strong> Include relevant background information</li>
        <li><strong>Use examples:</strong> Few-shot prompting improves performance</li>
        <li><strong>Break down tasks:</strong> Decompose complex problems</li>
        <li><strong>Verify outputs:</strong> Always fact-check critical information</li>
        <li><strong>Iterate prompts:</strong> Refine based on responses</li>
        <li><strong>Set temperature:</strong> Lower for factual, higher for creative</li>
      </ul>
    </div>

    <h2>Summary</h2>

    <p>Large Language Models represent a paradigm shift in AI, achieving remarkable capabilities through scale and sophisticated training. Key points:</p>

    <ul>
      <li>Built on Transformer architecture, scaled to billions/trillions of parameters</li>
      <li>Three-phase training: pre-training, SFT, RLHF</li>
      <li>Emergent abilities appear only at sufficient scale</li>
      <li>Effective prompting is critical for good results</li>
      <li>Still have significant limitations (hallucinations, reasoning, grounding)</li>
    </ul>

    <div style="margin-top: 3rem; padding-top: 2rem; border-top: 2px solid var(--color-border); display: flex; justify-content: space-between;">
      <a href="/chapters/04-transformers" class="btn btn-secondary">â† Previous: Transformers</a>
      <a href="/chapters/06-generative-ai" class="btn btn-primary">Next: Generative AI â†’</a>
    </div>
  </article>
</BaseLayout>
