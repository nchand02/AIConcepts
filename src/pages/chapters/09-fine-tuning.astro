---
import BaseLayout from '../../layouts/BaseLayout.astro';
---

<BaseLayout 
  title="Chapter 9: Fine-Tuning & PEFT" 
  description="Fine-tuning LLMs efficiently with LoRA, QLoRA, and Parameter-Efficient Fine-Tuning methods"
  currentPage="/chapters/09-fine-tuning"
>
  <article>
    <h1>Chapter 9: Fine-Tuning & PEFT</h1>
    
    <div class="callout info">
      <div class="callout-title">ðŸ“š Chapter Overview</div>
      <p>Fine-tuning adapts pre-trained models to specific tasks or domains. This chapter covers full fine-tuning, parameter-efficient methods like LoRA, and instruction tuning techniques.</p>
    </div>

    <div class="callout warning">
      <div class="callout-title">ðŸš§ Content In Progress</div>
      <p>This chapter is currently being developed. Check back soon for comprehensive content on fine-tuning techniques.</p>
    </div>

    <h2>Topics to be Covered</h2>
    
    <ul>
      <li><strong>Full Fine-Tuning</strong> - Traditional approach and challenges</li>
      <li><strong>Parameter-Efficient Fine-Tuning (PEFT)</strong> - Overview and benefits</li>
      <li><strong>LoRA & QLoRA</strong> - Low-rank adaptation techniques</li>
      <li><strong>Prefix Tuning & P-Tuning</strong> - Prompt-based fine-tuning</li>
      <li><strong>Adapter Layers</strong> - Modular fine-tuning approach</li>
      <li><strong>Instruction Tuning</strong> - Creating instruction-following models</li>
      <li><strong>RLHF & DPO</strong> - Alignment and preference learning</li>
    </ul>

    <h2>Coming Soon</h2>
    <p>This chapter will include practical guides for fine-tuning models on custom datasets with various techniques.</p>

  </article>
</BaseLayout>
