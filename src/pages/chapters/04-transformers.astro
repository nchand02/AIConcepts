---
import BaseLayout from '../../layouts/BaseLayout.astro';
---

<BaseLayout 
  title="Chapter 4: Transformers" 
  description="Deep dive into Transformer architecture, attention mechanisms, and their revolutionary impact on AI"
  currentPage="/chapters/04-transformers"
>
  <article>
    <h1>Chapter 4: Transformers</h1>
    
    <div class="callout info">
      <div class="callout-title">üìö Chapter Overview</div>
      <p>Transformers revolutionized AI in 2017 with the "Attention Is All You Need" paper. This chapter explores the architecture that powers modern LLMs, from BERT to GPT to every major AI breakthrough since.</p>
    </div>

    <h2>The Transformer Revolution</h2>
    
    <p>Before Transformers, sequential processing dominated NLP through RNNs and LSTMs. These architectures had fundamental limitations:</p>

    <ul>
      <li><strong>Sequential processing:</strong> Couldn't parallelize ‚Üí slow training</li>
      <li><strong>Limited context:</strong> Struggled with long-range dependencies</li>
      <li><strong>Vanishing gradients:</strong> Information loss across long sequences</li>
    </ul>

    <p>Transformers solved these problems by replacing recurrence with <strong>self-attention</strong>, enabling parallel processing and unlimited context windows.</p>

    <h2>Core Architecture</h2>

    <div class="diagram-container">
      <p class="diagram-title">Transformer Architecture Overview</p>
      <pre class="mermaid" role="img" aria-label="Transformer architecture showing input tokens flowing through embeddings, positional encoding, encoder stack, decoder stack, and linear softmax to output probabilities" set:html={`graph TB
    A[Input Tokens] --> B[Input Embeddings]
    B --> C[Positional Encoding]
    C --> D[Encoder Stack<br/>N √ó layers]
    D --> E[Decoder Stack<br/>N √ó layers]
    E --> F[Linear + Softmax]
    F --> G[Output Probabilities]
    
    %% Semantic color classes
    classDef input fill:#dbeafe,stroke:#3b82f6,stroke-width:2px,color:#1e293b
    classDef encoder fill:#fef3c7,stroke:#f59e0b,stroke-width:2px,color:#1e293b
    classDef decoder fill:#fce7f3,stroke:#ec4899,stroke-width:2px,color:#1e293b
    classDef output fill:#d1fae5,stroke:#10b981,stroke-width:2px,color:#1e293b
    
    class A,B,C input
    class D encoder
    class E decoder
    class F,G output
      `} />
    </div>

    <h3>High-Level Components</h3>

    <table>
      <thead>
        <tr>
          <th>Component</th>
          <th>Purpose</th>
          <th>Key Features</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><strong>Encoder</strong></td>
          <td>Process and understand input</td>
          <td>Bidirectional attention, N stacked layers</td>
        </tr>
        <tr>
          <td><strong>Decoder</strong></td>
          <td>Generate output sequence</td>
          <td>Masked attention, encoder-decoder attention</td>
        </tr>
        <tr>
          <td><strong>Attention</strong></td>
          <td>Focus on relevant parts</td>
          <td>Self-attention, multi-head, parallelizable</td>
        </tr>
        <tr>
          <td><strong>Positional Encoding</strong></td>
          <td>Inject sequence order</td>
          <td>Sinusoidal or learned embeddings</td>
        </tr>
        <tr>
          <td><strong>Feed-Forward</strong></td>
          <td>Process attention outputs</td>
          <td>2-layer MLP with ReLU/GELU</td>
        </tr>
      </tbody>
    </table>

    <h2>Self-Attention Mechanism</h2>

    <div class="callout info">
      <div class="callout-title">üìö Historical Evolution of Attention (1992-2025)</div>
      <p><strong>Early foundations (1992-2005):</strong> Fast weight controllers (Schmidhuber 1992) anticipated key-value mechanisms. Bilateral filtering (1998) introduced pairwise affinity matrices for image processing.</p>
      <p><strong>RNN era (2014-2016):</strong> Bahdanau et al. (2014) introduced attention for seq2seq translation, solving the bottleneck of encoding entire sequences into fixed vectors. Attention extended to vision for image captioning (2015), then self-attention integrated into RNN models (2016).</p>
      <p><strong>Transformer breakthrough (2017):</strong> "Attention Is All You Need" formalized scaled dot-product self-attention, eliminating recurrence entirely. This enabled parallel processing and became the foundation for BERT, GPT, T5, and all modern LLMs.</p>
      <p><strong>Modern era (2019-2025):</strong> Efficient transformers (Reformer 2019, Linformer 2020, Performer 2020) addressed O(n¬≤) complexity. Flash Attention (2022) revolutionized efficiency through memory-aware algorithms. Vision Transformers (ViT 2020) and AlphaFold (2021) demonstrated attention's power beyond NLP. By 2023-2025, attention mechanisms power virtually all state-of-the-art AI systems across modalities.</p>
      <p><strong>Wikipedia reference:</strong> <a href="https://en.wikipedia.org/wiki/Attention_(machine_learning)" target="_blank">Attention (machine learning)</a> - comprehensive timeline from 1950s cognitive science through 2025</p>
    </div>

    <h3>The Core Innovation</h3>
    
    <p>Self-attention allows each token to attend to all other tokens in the sequence, computing relevance scores dynamically:</p>

    <div class="diagram-container">
      <pre class="ascii-diagram">
Input: "The cat sat on the mat"

Attention weights (example for "cat"):
      The    cat    sat    on    the    mat
The   0.05   0.10   0.05   0.02   0.03   0.01
cat   0.10   0.50   0.20   0.05   0.08   0.07  ‚Üê High attention to "cat" itself
sat   0.05   0.20   0.50   0.15   0.05   0.05
on    0.02   0.05   0.15   0.55   0.13   0.10
the   0.03   0.08   0.05   0.13   0.50   0.21
mat   0.01   0.07   0.05   0.10   0.21   0.56

Each row shows what that word attends to
      </pre>
    </div>

    <h3>Attention Formula</h3>

    <p>The mathematical foundation of attention:</p>

    <pre set:html={`<code>Attention(Q, K, V) = softmax(QK^T / sqrtd_k) √ó V

Where:
- Q (Query): What am I looking for?
- K (Key): What do I contain?
- V (Value): What do I actually represent?
- d_k: Dimension of keys (for scaling)</code>`} />

    <div class="collapsible">
      <div class="collapsible-header">
        <span>üîç Deep Dive: How Attention Computes</span>
        <span class="collapsible-icon">‚ñº</span>
      </div>
      <div class="collapsible-content">
        <p><strong>Step-by-step process:</strong></p>
        
        <pre set:html={`<code>Given input embeddings X (shape: seq_len √ó d_model):

1. Create Q, K, V through linear projections:
   Q = X √ó W_Q  (shape: seq_len √ó d_k)
   K = X √ó W_K  (shape: seq_len √ó d_k)
   V = X √ó W_V  (shape: seq_len √ó d_v)

2. Compute attention scores:
   scores = Q √ó K^T  (shape: seq_len √ó seq_len)
   # Each position gets a score for every other position

3. Scale scores:
   scaled_scores = scores / sqrtd_k
   # Prevents softmax saturation for large d_k

4. Apply softmax to get attention weights:
   attention_weights = softmax(scaled_scores)
   # Converts scores to probabilities (sum to 1)

5. Weighted sum of values:
   output = attention_weights √ó V
   # Each position is now a weighted combination of all values</code>`} />

        <p><strong>Example with numbers:</strong></p>
        
        <pre set:html={`<code>Input: ["I", "love", "AI"]
Embeddings (simplified to 2D):
I    = [1.0, 0.5]
love = [0.3, 0.9]
AI   = [0.7, 0.2]

After projections:
Q_I    = [0.8, 0.4]
K_I    = [0.9, 0.3]
V_I    = [1.1, 0.6]
... (similarly for "love" and "AI")

Attention score for "I" attending to "love":
score = Q_I ¬∑ K_love = (0.8 √ó 0.5) + (0.4 √ó 1.0) = 0.8
scaled = 0.8 / sqrt2 = 0.566

After softmax across all positions:
"I" attends to: {{"I": 0.42, "love": 0.38, "AI": 0.20}}

Final output for "I":
output_I = 0.42√óV_I + 0.38√óV_love + 0.20√óV_AI</code>`} />
      </div>
    </div>

    <h3>Multi-Head Attention</h3>

    <p>Instead of single attention, use multiple "heads" to capture different relationships:</p>

    <div class="diagram-container">
      <p class="diagram-title">Multi-Head Attention Mechanism</p>
      <pre class="mermaid" role="img" aria-label="Multi-head attention showing input embeddings split across 8 heads focusing on syntax, semantics, position, and context, then concatenated and projected to output" set:html={`graph TB
    A[Input Embeddings] --> B1[Head 1<br/>Syntax focus]
    A --> B2[Head 2<br/>Semantics focus]
    A --> B3[Head 3<br/>Position focus]
    A --> B4[Head 8<br/>Context focus]
    
    B1 --> C[Concatenate]
    B2 --> C
    B3 --> C
    B4 --> C
    
    C --> D[Linear Projection]
    D --> E[Output]
    
    %% Semantic color classes
    classDef input fill:#dbeafe,stroke:#3b82f6,stroke-width:2px,color:#1e293b
    classDef head fill:#ede9fe,stroke:#8b5cf6,stroke-width:2px,color:#1e293b
    classDef combine fill:#fef3c7,stroke:#f59e0b,stroke-width:2px,color:#1e293b
    classDef output fill:#d1fae5,stroke:#10b981,stroke-width:2px,color:#1e293b
    
    class A input
    class B1,B2,B3,B4 head
    class C,D combine
    class E output
      `} />
      <p class="diagram-caption"><em>Each attention head learns different aspects of relationships between tokens</em></p>
    </div>

    <p><strong>Formula:</strong></p>
    
    <pre set:html={`<code>MultiHead(Q, K, V) = Concat(head_1, ..., head_h) √ó W_O

where head_i = Attention(Q√óW_Q^i, K√óW_K^i, V√óW_V^i)

Typical configuration:
- h = 8 or 16 heads
- d_model = 512 or 768
- d_k = d_v = d_model / h</code>`} />

    <h3>Why Multi-Head?</h3>

    <ul>
      <li><strong>Capture multiple relationships:</strong> Syntax, semantics, position, context</li>
      <li><strong>Ensemble effect:</strong> Different heads specialize in different patterns</li>
      <li><strong>Richer representations:</strong> More expressive than single attention</li>
      <li><strong>Computational efficiency:</strong> Smaller d_k per head allows parallelization</li>
    </ul>

    <h2>Positional Encoding</h2>

    <p>Since Transformers process all tokens in parallel, they have no inherent notion of order. Positional encodings inject sequence information:</p>

    <h3>Sinusoidal Positional Encoding (Original)</h3>

    <pre set:html={`<code>PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))

where:
- pos = token position (0, 1, 2, ...)
- i = dimension index
- d_model = embedding dimension</code>`} />

    <div class="diagram-container">
      <pre class="ascii-diagram">
Visualization (each row = position, each column = dimension):

Pos 0: ‚ñì‚ñì‚ñì‚ñì‚ñì‚ñë‚ñë‚ñë‚ñë‚ñë‚ñì‚ñì‚ñì‚ñì‚ñì‚ñë‚ñë‚ñë‚ñë‚ñë‚ñì‚ñì‚ñì‚ñì‚ñì‚ñë‚ñë‚ñë‚ñë‚ñë  (smooth waves)
Pos 1: ‚ñì‚ñì‚ñì‚ñì‚ñë‚ñë‚ñë‚ñë‚ñë‚ñì‚ñì‚ñì‚ñì‚ñë‚ñë‚ñë‚ñë‚ñë‚ñì‚ñì‚ñì‚ñì‚ñë‚ñë‚ñë‚ñë‚ñë‚ñì‚ñì
Pos 2: ‚ñì‚ñì‚ñì‚ñë‚ñë‚ñë‚ñë‚ñë‚ñì‚ñì‚ñì‚ñë‚ñë‚ñë‚ñë‚ñë‚ñì‚ñì‚ñì‚ñë‚ñë‚ñë‚ñë‚ñë‚ñì‚ñì‚ñì‚ñë‚ñë
Pos 3: ‚ñì‚ñì‚ñë‚ñë‚ñë‚ñë‚ñë‚ñì‚ñì‚ñë‚ñë‚ñë‚ñë‚ñë‚ñì‚ñì‚ñë‚ñë‚ñë‚ñë‚ñë‚ñì‚ñì‚ñë‚ñë‚ñë‚ñë‚ñë‚ñì
...

Different frequencies for different dimensions
      </pre>
    </div>

    <h3>Learned Positional Embeddings (BERT, GPT)</h3>

    <p>Modern models often use learned embeddings instead:</p>

    <pre set:html={`<code># Learned position embeddings
position_embeddings = nn.Embedding(max_seq_len, d_model)

# Add to token embeddings
combined = token_embeddings + position_embeddings(positions)</code>`} />

    <p><strong>Advantages:</strong></p>
    <ul>
      <li>Can adapt to specific tasks</li>
      <li>May capture task-specific position patterns</li>
      <li>Used in most modern models (GPT, BERT, T5)</li>
    </ul>

    <h2>Encoder Architecture</h2>

    <p>The encoder processes input to create contextual representations:</p>

    <div class="diagram-container">
      <p class="diagram-title">Encoder Layer Structure</p>
      <pre class="mermaid" role="img" aria-label="Encoder layer showing input with positional encoding through multi-head self-attention, add-and-norm, feed-forward network, and another add-and-norm, repeated N times" set:html={`graph TB
    A[Input + Positional Encoding] --> B[Multi-Head<br/>Self-Attention]
    B --> C[Add & Norm]
    C --> D[Feed-Forward<br/>Network]
    D --> E[Add & Norm]
    E --> F{Repeat N times}
    F -->|Yes| B
    F -->|No| G[Encoder Output]
    
    %% Semantic color classes
    classDef input fill:#dbeafe,stroke:#3b82f6,stroke-width:2px,color:#1e293b
    classDef attention fill:#ede9fe,stroke:#8b5cf6,stroke-width:2px,color:#1e293b
    classDef norm fill:#fef3c7,stroke:#f59e0b,stroke-width:2px,color:#1e293b
    classDef ff fill:#fce7f3,stroke:#ec4899,stroke-width:2px,color:#1e293b
    classDef decision fill:#f3f4f6,stroke:#6b7280,stroke-width:2px,color:#1e293b,stroke-dasharray: 5 5
    classDef output fill:#d1fae5,stroke:#10b981,stroke-width:2px,color:#1e293b
    
    class A input
    class B attention
    class C,E norm
    class D ff
    class F decision
    class G output
      `} />
    </div>

    <h3>Encoder Layer Details</h3>

    <pre set:html={`<code>class EncoderLayer:
    def forward(x):
        # 1. Multi-head self-attention
        attn_output = MultiHeadAttention(x, x, x)
        x = LayerNorm(x + attn_output)  # Residual + Norm
        
        # 2. Feed-forward network
        ff_output = FeedForward(x)
        x = LayerNorm(x + ff_output)    # Residual + Norm
        
        return x

class FeedForward:
    def forward(x):
        return W_2(GELU(W_1(x) + b_1)) + b_2
    # Typically: d_model ‚Üí 4√ód_model ‚Üí d_model</code>`} />

    <h3>Key Components</h3>

    <ul>
      <li><strong>Residual Connections:</strong> x + Sublayer(x) prevents vanishing gradients</li>
      <li><strong>Layer Normalization:</strong> Stabilizes training, normalizes across features</li>
      <li><strong>Feed-Forward:</strong> 2-layer MLP processes each position independently</li>
      <li><strong>GELU Activation:</strong> Smoother than ReLU, better gradients</li>
    </ul>

    <h2>Decoder Architecture</h2>

    <p>The decoder generates output autoregressively (one token at a time):</p>

    <div class="diagram-container">
      <p class="diagram-title">Decoder Layer Structure with Cross-Attention</p>
      <pre class="mermaid" role="img" aria-label="Decoder layer with masked self-attention, encoder-decoder cross-attention, feed-forward network, and add-and-norm layers, repeated N times" set:html={`graph TB
    A[Output Tokens<br/>+ Positional Encoding] --> B[Masked Multi-Head<br/>Self-Attention]
    B --> C[Add & Norm]
    C --> D[Encoder-Decoder<br/>Cross-Attention]
    D --> E[Add & Norm]
    E --> F[Feed-Forward<br/>Network]
    F --> G[Add & Norm]
    G --> H{Repeat N times}
    H -->|Yes| B
    H -->|No| I[Decoder Output]
    
    J[Encoder Output] --> D
    
    %% Semantic color classes
    classDef input fill:#dbeafe,stroke:#3b82f6,stroke-width:2px,color:#1e293b
    classDef attention fill:#ede9fe,stroke:#8b5cf6,stroke-width:2px,color:#1e293b
    classDef cross fill:#fce7f3,stroke:#ec4899,stroke-width:2px,color:#1e293b
    classDef norm fill:#fef3c7,stroke:#f59e0b,stroke-width:2px,color:#1e293b
    classDef decision fill:#f3f4f6,stroke:#6b7280,stroke-width:2px,color:#1e293b,stroke-dasharray: 5 5
    classDef output fill:#d1fae5,stroke:#10b981,stroke-width:2px,color:#1e293b
    
    class A,J input
    class B attention
    class D cross
    class C,E,G norm
    class F cross
    class H decision
    class I output
      `} />
      <p class="diagram-caption"><em>Cross-attention allows the decoder to attend to encoder outputs at every layer</em></p>
    </div>

    <h3>Masked Self-Attention</h3>

    <p>Prevents the decoder from "cheating" by looking at future tokens:</p>

    <div class="diagram-container">
      <pre class="ascii-diagram">
Attention mask (1 = allowed, 0 = masked):

       I  love  AI   !
I      1   0    0   0   ‚Üê Can only see "I"
love   1   1    0   0   ‚Üê Can see "I love"
AI     1   1    1   0   ‚Üê Can see "I love AI"
!      1   1    1   1   ‚Üê Can see all

Lower triangular matrix ensures causality
      </pre>
    </div>

    <pre set:html={`<code># Apply mask before softmax
scores = QK^T / sqrtd_k
scores = scores + mask  # mask: 0 for allowed, -‚àû for masked
attention_weights = softmax(scores)  # Masked positions become 0</code>`} />

    <h3>Cross-Attention</h3>

    <p>Links decoder to encoder output:</p>

    <pre set:html={`<code># Decoder attends to encoder
Q = decoder_hidden_states √ó W_Q  # Query from decoder
K = encoder_outputs √ó W_K        # Key from encoder  
V = encoder_outputs √ó W_V        # Value from encoder

attention = Attention(Q, K, V)   # Decoder "looks at" encoder</code>`} />

    <h2>Training Transformers</h2>

    <h3>Training Objective</h3>

    <p>For sequence-to-sequence tasks (translation, summarization):</p>

    <pre set:html={`<code>Loss = CrossEntropy(predicted_tokens, target_tokens)

# Teacher forcing: use ground truth as decoder input during training
for each position t:
    decoder_input_t = target_sequence[:t]
    prediction_t = model(encoder_input, decoder_input_t)
    loss += CrossEntropy(prediction_t, target_t)</code>`} />

    <h3>Learning Rate Schedule: Warmup</h3>

    <p>Critical for stable Transformer training:</p>

    <pre set:html={`<code>lr = d_model^(-0.5) √ó min(step^(-0.5), step √ó warmup_steps^(-1.5))

# Typical warmup: 4,000-10,000 steps
# Initial learning rate increases, then decays</code>`} />

    <div class="diagram-container">
      <pre class="ascii-diagram">
Learning Rate Schedule:

 LR ‚îÇ     ‚ï±‚ï≤
    ‚îÇ    ‚ï±  ‚ï≤___
    ‚îÇ   ‚ï±       ‚ï≤___
    ‚îÇ  ‚ï±            ‚ï≤___
    ‚îÇ ‚ï±                 ‚ï≤___
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Step
      ‚Üë                    
   Warmup              Decay
   (4k steps)
      </pre>
    </div>

    <h3>Label Smoothing</h3>

    <p>Prevents overconfidence, improves generalization:</p>

    <pre set:html={`<code># Instead of hard targets [0, 0, 1, 0, 0]
# Use smoothed targets [0.025, 0.025, 0.9, 0.025, 0.025]

smoothed_label = (1 - smoothing) √ó true_label + smoothing / num_classes
# Typical smoothing: 0.1</code>`} />

    <h2>Transformer Variants</h2>

    <h3>Encoder-Only Models (BERT family)</h3>

    <p><strong>Architecture:</strong> Only encoder stack, bidirectional attention</p>
    
    <p><strong>Use cases:</strong> Classification, named entity recognition, question answering</p>

    <table>
      <thead>
        <tr>
          <th>Model</th>
          <th>Key Feature</th>
          <th>Training Task</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><strong><a href="https://en.wikipedia.org/wiki/BERT_(language_model)" target="_blank">BERT</a></strong> (2018)</td>
          <td>Bidirectional context, 110M-340M params</td>
          <td>Masked language modeling (15% tokens) + Next Sentence Prediction. Google adopted for 70+ languages by Dec 2019, processing almost every English query by Oct 2020</td>
        </tr>
        <tr>
          <td><strong>RoBERTa</strong></td>
          <td>Improved BERT</td>
          <td>Dynamic masking, larger batches</td>
        </tr>
        <tr>
          <td><strong>ALBERT</strong></td>
          <td>Parameter sharing</td>
          <td>Factorized embeddings</td>
        </tr>
        <tr>
          <td><strong>DeBERTa</strong></td>
          <td>Disentangled attention</td>
          <td>Separate content & position</td>
        </tr>
      </tbody>
    </table>

    <h3>Decoder-Only Models (GPT family)</h3>

    <p><strong>Architecture:</strong> Only decoder stack, causal (left-to-right) attention</p>
    
    <p><strong>Use cases:</strong> Text generation, completion, few-shot learning</p>

    <table>
      <thead>
        <tr>
          <th>Model</th>
          <th>Parameters</th>
          <th>Key Innovation</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><strong>GPT</strong> (Jun 2018)</td>
          <td>117M</td>
          <td>First decoder-only transformer demonstrating unsupervised pre-training + fine-tuning paradigm. Trained on BooksCorpus (7,000 unpublished books)</td>
        </tr>
        <tr>
          <td><strong>GPT-2</strong> (Feb 2019)</td>
          <td>1.5B (largest)</td>
          <td>Zero-shot task transfer, initially held back due to concerns about misuse. 10√ó larger than GPT, trained on WebText (8M documents)</td>
        </tr>
        <tr>
          <td><strong>GPT-3</strong> (May 2020)</td>
          <td>175B</td>
          <td>In-context learning breakthrough - few-shot prompting without fine-tuning. 100√ó larger than GPT-2, trained on 570GB text (CommonCrawl, WebText2, Books, Wikipedia)</td>
        </tr>
        <tr>
          <td><strong>GPT-4</strong> (Mar 2023)</td>
          <td>Estimated 1.7T+ (mixture of experts)</td>
          <td>Multimodal (text+images), 32K-128K context, human-level performance on many benchmarks. Passes bar exam (90th percentile), outperforms GPT-3.5 on complex reasoning</td>
        </tr>
        <tr>
          <td><strong>GPT-4 Turbo/o1</strong> (2024-2025)</td>
          <td>Optimized variants</td>
          <td>GPT-4 Turbo: 128K context, cheaper, faster. o1 series: Advanced reasoning with "chain of thought" for complex problem-solving (2024-2025)</td>
        </tr>
      </tbody>
    </table>

    <h3>Encoder-Decoder Models (T5, BART)</h3>

    <p><strong>Architecture:</strong> Full transformer (encoder + decoder)</p>
    
    <p><strong>Use cases:</strong> Translation, summarization, question answering</p>

    <ul>
      <li><strong>T5:</strong> "Text-to-Text Transfer Transformer" - treats all tasks as text generation</li>
      <li><strong>BART:</strong> Combines BERT and GPT training objectives</li>
      <li><strong>mT5:</strong> Multilingual T5</li>
    </ul>

    <h2>Computational Complexity</h2>

    <h3>Time and Memory</h3>

    <table>
      <thead>
        <tr>
          <th>Operation</th>
          <th>Complexity</th>
          <th>Notes</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><strong>Self-Attention</strong></td>
          <td>O(n¬≤ √ó d)</td>
          <td>Quadratic in sequence length</td>
        </tr>
        <tr>
          <td><strong>Feed-Forward</strong></td>
          <td>O(n √ó d¬≤)</td>
          <td>Linear in sequence length</td>
        </tr>
        <tr>
          <td><strong>Total per layer</strong></td>
          <td>O(n¬≤ √ó d + n √ó d¬≤)</td>
          <td>Bottleneck: n¬≤ for long sequences</td>
        </tr>
      </tbody>
    </table>

    <div class="callout warning">
      <div class="callout-title">‚ö†Ô∏è The n¬≤ Problem</div>
      <p>Self-attention's quadratic complexity becomes prohibitive for long sequences. For n=10,000 tokens, attention matrix has 100 million elements!</p>
    </div>

    <h3>Efficient Attention Variants</h3>

    <ul>
      <li><strong>Sparse Attention:</strong> Only attend to subset of positions</li>
      <li><strong>Linformer:</strong> Projects keys/values to lower dimension ‚Üí O(n)</li>
      <li><strong>Performer:</strong> Approximates attention with kernels ‚Üí O(n)</li>
      <li><strong><a href="https://en.wikipedia.org/wiki/Attention_(machine_learning)#Flash_attention" target="_blank">Flash Attention</a> (2022):</strong> IO-aware algorithm that partitions attention computation into smaller blocks fitting GPU on-chip memory, reducing memory usage while increasing efficiency 2-4√ó without sacrificing accuracy. Addresses the memory bottleneck of storing large intermediate attention matrices</li>
      <li><strong>Multi-Query Attention:</strong> Share keys/values across heads</li>
      <li><strong>Sliding Window:</strong> Local attention within fixed window</li>
    </ul>

    <h2>Advanced Topics</h2>

    <h3>Relative Position Encodings</h3>

    <p>Instead of absolute positions, encode relative distances:</p>

    <pre set:html={`<code># T5 approach: Bias attention based on distance
attention_bias[i, j] = learned_bias[clip(i - j, -k, k)]

# Advantages:
- Better generalization to longer sequences
- Captures relative relationships
- Used in T5, Transformer-XL</code>`} />

    <h3>Grouped Query Attention (GQA)</h3>

    <p>Balance between Multi-Head and Multi-Query attention:</p>

    <pre set:html={`<code>Multi-Head:   8 heads √ó (Q, K, V) = 24 projections
Multi-Query:  8 heads √ó Q, 1 √ó (K, V) = 10 projections  
Grouped:      8 heads √ó Q, 2 groups √ó (K, V) = 12 projections

# Used in Llama 2, PaLM
# Reduces memory, maintains quality</code>`} />

    <h3>Mixture of Experts (MoE)</h3>

    <p>Conditional computation - activate subset of parameters:</p>

    <div class="diagram-container">
      <p class="diagram-title">Mixture of Experts (MoE) Architecture</p>
      <pre class="mermaid" role="img" aria-label="Mixture of experts showing input routed to top-K experts among 8 specialists, with weighted combination of expert outputs" set:html={`graph TB
    A[Input] --> B[Routing Network]
    B --> C{Top-K Expert Selection}
    C -->|Weight w1| D1[Expert 1]
    C -->|Weight w2| D2[Expert 2]
    C -->|Weight w8| D3[Expert 8]
    D1 --> E[Weighted Combine]
    D2 --> E
    D3 --> E
    E --> F[Output]
    
    %% Semantic color classes
    classDef input fill:#dbeafe,stroke:#3b82f6,stroke-width:2px,color:#1e293b
    classDef router fill:#ede9fe,stroke:#8b5cf6,stroke-width:2px,color:#1e293b
    classDef decision fill:#f3f4f6,stroke:#6b7280,stroke-width:2px,color:#1e293b,stroke-dasharray: 5 5
    classDef expert fill:#fef3c7,stroke:#f59e0b,stroke-width:2px,color:#1e293b
    classDef combine fill:#fce7f3,stroke:#ec4899,stroke-width:2px,color:#1e293b
    classDef output fill:#d1fae5,stroke:#10b981,stroke-width:2px,color:#1e293b
    
    class A input
    class B router
    class C decision
    class D1,D2,D3 expert
    class E combine
    class F output
      `} />
      <p class="diagram-caption"><em>Only a subset of experts are activated per input, enabling efficient parameter scaling</em></p>
    </div>

    <p><strong>Benefits:</strong></p>
    <ul>
      <li>Massive parameter scaling (trillions) without proportional compute</li>
      <li>Specialization: different experts for different inputs</li>
      <li>Used in: Switch Transformer, GLaM, GPT-4 (rumored)</li>
    </ul>

    <h2>Implementation Tips</h2>

    <div class="collapsible">
      <div class="collapsible-header">
        <span>üíª Code Example: Minimal Transformer</span>
        <span class="collapsible-icon">‚ñº</span>
      </div>
      <div class="collapsible-content">
        <pre set:html={`<code>import torch
import torch.nn as nn
import math

class MultiHeadAttention(nn.Module):
    """
    Multi-head attention from "Attention Is All You Need" (Vaswani et al., 2017).
    
    Core innovation: Multiple parallel attention heads capture different relationship types
    (syntax, semantics, position, context) simultaneously. Each head operates on a reduced
    dimension (d_k = d_model / num_heads), enabling efficient parallel computation while
    maintaining expressiveness through the ensemble of heads.
    
    Args:
        d_model: Model dimension (typically 512 or 768)
        num_heads: Number of parallel attention heads (typically 8 or 12)
    """
    def __init__(self, d_model, num_heads):
        super().__init__()
        assert d_model % num_heads == 0
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads  # Per-head dimension
        
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        
    def forward(self, Q, K, V, mask=None):
        batch_size = Q.size(0)
        
        # Linear projections: Transform inputs to Query, Key, Value representations
        # Then reshape to (batch, num_heads, seq_len, d_k) for parallel head processing
        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        
        # Scaled dot-product attention: Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) V
        # QK^T computes similarity scores between all query-key pairs (seq_len √ó seq_len matrix)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        # Scaling by sqrt(d_k) prevents softmax saturation for large dimensions
        
        if mask is not None:
            # Masking: Set illegal positions to -inf before softmax (becomes 0 after softmax)
            # Used for causal/autoregressive decoding (prevent attending to future tokens)
            scores = scores.masked_fill(mask == 0, -1e9)
        
        # Softmax converts scores to attention weights (probabilities summing to 1)
        attention = torch.softmax(scores, dim=-1)
        # Weighted sum: Each output token is a weighted combination of all value vectors
        output = torch.matmul(attention, V)
        
        # Concatenate heads: Reshape from (batch, heads, seq_len, d_k) ‚Üí (batch, seq_len, d_model)
        # Then project through W_o to produce final output
        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)
        return self.W_o(output)

class FeedForward(nn.Module):
    def __init__(self, d_model, d_ff=2048, dropout=0.1):
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.dropout = nn.Dropout(dropout)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.activation = nn.GELU()
        
    def forward(self, x):
        return self.linear2(self.dropout(self.activation(self.linear1(x))))

class TransformerBlock(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        self.attention = MultiHeadAttention(d_model, num_heads)
        self.feed_forward = FeedForward(d_model, d_ff, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x, mask=None):
        # Self-attention with residual connection
        attn_output = self.attention(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))
        
        # Feed-forward with residual connection
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))
        
        return x</code>`} />
      </div>
    </div>

    <h2>Practical Applications</h2>

    <div class="callout success">
      <div class="callout-title">üöÄ Modern Transformers (2023-2025)</div>
      <p>By 2023-2025, transformers have become the universal architecture for AI. <strong>Multimodal models</strong> like GPT-4, Gemini, and Claude 3 process text, images, audio, and video using unified transformer architectures. <strong>Constitutional AI and RLHF</strong> (Reinforcement Learning from Human Feedback) align models with human values and preferences. <strong>Scaling laws</strong> demonstrate predictable improvements with model size, leading to trillion-parameter models. <strong>Long-context transformers</strong> now handle 100K-1M+ token contexts, enabling entire codebases and books as input.</p>
      <p>Transformers power Google Search (BERT since 2020), GitHub Copilot, ChatGPT, medical diagnosis systems, protein folding (AlphaFold Nobel Prize 2024), and creative applications (DALL-E, Midjourney, Stable Diffusion). The architecture has expanded beyond its NLP origins to dominate computer vision, speech, robotics, and scientific discovery.</p>
      <p><strong>Wikipedia references:</strong> <a href="https://en.wikipedia.org/wiki/BERT_(language_model)" target="_blank">BERT</a>, <a href="https://en.wikipedia.org/wiki/Attention_(machine_learning)" target="_blank">Attention mechanism</a></p>
    </div>

    <div class="card-grid">
      <div class="card">
        <div class="card-title">üìù Natural Language Processing</div>
        <div class="card-description">
          Translation, summarization, question answering, sentiment analysis
        </div>
      </div>

      <div class="card">
        <div class="card-title">üí¨ Conversational AI</div>
        <div class="card-description">
          Chatbots, virtual assistants, customer service automation
        </div>
      </div>

      <div class="card">
        <div class="card-title">üñºÔ∏è Vision Transformers</div>
        <div class="card-description">
          Image classification, object detection, image generation (DALL-E)
        </div>
      </div>

      <div class="card">
        <div class="card-title">üéµ Audio Processing</div>
        <div class="card-description">
          Speech recognition, music generation, audio classification
        </div>
      </div>

      <div class="card">
        <div class="card-title">üß¨ Protein Folding</div>
        <div class="card-description">
          AlphaFold uses attention for 3D structure prediction
        </div>
      </div>

      <div class="card">
        <div class="card-title">üíª Code Generation</div>
        <div class="card-description">
          GitHub Copilot, code completion, bug fixing
        </div>
      </div>
    </div>

    <h2>Best Practices</h2>

    <div class="callout success">
      <div class="callout-title">‚úÖ Training Tips</div>
      <ul>
        <li><strong>Learning rate warmup:</strong> Critical for stable training</li>
        <li><strong>Gradient clipping:</strong> Prevent exploding gradients (clip at 1.0)</li>
        <li><strong>Mixed precision:</strong> Use FP16 with loss scaling</li>
        <li><strong>Large batch sizes:</strong> 256-4096+ for better convergence</li>
        <li><strong>Layer normalization:</strong> Pre-norm often more stable than post-norm</li>
        <li><strong>Label smoothing:</strong> Improves calibration and generalization</li>
      </ul>
    </div>

    <h2>Summary</h2>

    <p>Transformers revolutionized AI through several key innovations:</p>

    <ul>
      <li><strong>Self-attention:</strong> Enables parallelization and unlimited context</li>
      <li><strong>Multi-head attention:</strong> Captures diverse relationships</li>
      <li><strong>Positional encoding:</strong> Injects sequence information</li>
      <li><strong>Residual connections + LayerNorm:</strong> Enables very deep networks</li>
      <li><strong>Architecture variants:</strong> Encoder-only (BERT), Decoder-only (GPT), Encoder-Decoder (T5)</li>
    </ul>

    <p>The Transformer architecture underlies virtually all modern LLMs and has expanded beyond NLP into vision, audio, and multimodal applications.</p>

    <h2>Review Questions</h2>

    <ol>
      <li>Why is self-attention more powerful than RNNs for long sequences?</li>
      <li>What is the purpose of the scaling factor (sqrtd_k) in attention?</li>
      <li>How does multi-head attention differ from single-head attention?</li>
      <li>Why do we need positional encodings in Transformers?</li>
      <li>What is the difference between encoder-only and decoder-only Transformers?</li>
      <li>How does masked attention enable autoregressive generation?</li>
      <li>What causes the quadratic complexity in self-attention?</li>
    </ol>

    <h2>Practical Exercises</h2>

    <ol>
      <li>Implement a basic Transformer from scratch in PyTorch</li>
      <li>Visualize attention weights for a pre-trained model</li>
      <li>Compare sinusoidal vs. learned positional encodings</li>
      <li>Experiment with different numbers of attention heads</li>
      <li>Implement Flash Attention or another efficient variant</li>
      <li>Fine-tune a pre-trained Transformer on a custom dataset</li>
    </ol>

    <div style="margin-top: 3rem; padding-top: 2rem; border-top: 2px solid var(--color-border); display: flex; justify-content: space-between;">
      <a href="/chapters/03-deep-learning" class="btn btn-secondary">‚Üê Previous: Deep Learning</a>
      <a href="/chapters/05-llms" class="btn btn-primary">Next: Large Language Models ‚Üí</a>
    </div>
  </article>
</BaseLayout>
