---
import BaseLayout from '../../layouts/BaseLayout.astro';
---

<BaseLayout 
  title="Chapter 6: Generative AI" 
  description="Comprehensive guide to Generative AI, including GANs, VAEs, Diffusion Models, and image generation"
  currentPage="/chapters/06-generative-ai"
>
  <article>
    <h1>Chapter 6: Generative AI</h1>
    
    <div class="callout info">
      <div class="callout-title">ðŸ“š Chapter Overview</div>
      <p>Generative AI encompasses models that create new contentâ€”images, text, audio, and more. This chapter covers GANs, VAEs, diffusion models, and the technologies behind DALL-E, Stable Diffusion, and Midjourney.</p>
    </div>

    <div class="callout info">
      <div class="callout-title">ðŸŽ¨ Generative AI Evolution (2014-2025)</div>
      <p><strong>Generative AI revolutionized content creation from adversarial training to diffusion-based approaches dominating by 2024.</strong></p>
      <ul>
        <li><strong>GAN Era (2014-2021):</strong> June 2014 Goodfellow et al. introduced Generative Adversarial Networks at NeurIPS with generator-discriminator adversarial training â†’ 2015 DCGAN (Radford et al.) stabilized training with convolutional architecture â†’ 2016 WGAN (Arjovsky et al.) solved mode collapse using Wasserstein distance â†’ 2017 Progressive GAN (Karras et al.) gradually increased resolution 4Ã—4â†’1024Ã—1024 â†’ 2019 StyleGAN introduced style latent space with AdaIN â†’ 2020 StyleGAN-2 solved blob artifacts â†’ 2021 StyleGAN-3 eliminated texture sticking via Nyquist-Shannon theorem. Training challenges: unstable convergence, mode collapse, vanishing gradients.</li>
        <li><strong>Diffusion Breakthrough (2015-2024):</strong> 2015 Sohl-Dickstein et al. introduced diffusion using non-equilibrium thermodynamics â†’ June 2020 DDPM (Ho et al.) foundational work requiring ~1000 steps â†’ October 2020 DDIM (Song et al.) enabled 10-50Ã— faster sampling â†’ 2021 classifier guidance (Dhariwal & Nichol) used CLIP for text conditioning â†’ April 2022 Latent Diffusion (Rombach et al.) operated in VAE compressed space â†’ July 2022 Classifier-Free Guidance (Ho & Salimans) eliminated separate classifier â†’ 2023 Diffusion Transformers (DiT) replaced U-Net backbones.</li>
        <li><strong>Text-to-Image Revolution (2021-2024):</strong> 2021 DALL-E original (12B autoregressive Transformer) â†’ April 2022 DALL-E 2 (3.5B cascaded diffusion with unCLIP) â†’ May 2022 Midjourney V1 â†’ August 2022 Stable Diffusion (860M open-source democratizing access) â†’ 2022 Imagen (Google T5-XXL + cascaded diffusion) â†’ October 2023 DALL-E 3 â†’ December 2023 Midjourney V6 â†’ March 2024 Stable Diffusion 3 (DiT + rectified flow).</li>
        <li><strong>Video Generation (2022-2024):</strong> September 2022 Make-A-Video (Meta) â†’ March 2023 Runway Gen-2 â†’ February 2024 Sora (OpenAI DiT) â†’ October 2024 Movie Gen (Meta DiT series) â†’ 2024 Veo (Google).</li>
        <li><strong>Control & Fine-tuning (2023-2024):</strong> 2023 ControlNet added spatial conditioning for precise control â†’ LoRA enabled fast fine-tuning with minimal parameters â†’ 2023 Consistency Models (Song et al.) enabled 1-step generation â†’ Rectified Flow (Liu et al. September 2022) learned straight-line trajectories.</li>
      </ul>
      <p><em>Sources: <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network" target="_blank">Wikipedia - Generative Adversarial Network</a>, <a href="https://en.wikipedia.org/wiki/Diffusion_model" target="_blank">Wikipedia - Diffusion Model</a></em></p>
    </div>

    <h2>What is Generative AI?</h2>
    
    <p>Generative AI creates new content (images, text, audio, video) by learning patterns from training data. Unlike discriminative models that classify, generative models learn the underlying distribution of data to sample new instances.</p>

    <h3>Key Characteristics</h3>
    <ul>
      <li><strong>Content Creation:</strong> Generate novel data similar to training examples</li>
      <li><strong>Distribution Learning:</strong> Model probability distributions P(X)</li>
      <li><strong>Conditional Generation:</strong> Generate based on input conditions (prompts, images)</li>
      <li><strong>Controllability:</strong> Guide generation with various control mechanisms</li>
    </ul>

    <h2>Generative Adversarial Networks (GANs)</h2>

    <p>GANs consist of two networks competing in a minimax game. <strong>Introduced by Ian Goodfellow et al. in June 2014</strong> at NeurIPS, GANs revolutionized generative modeling through adversarial training in a zero-sum game framework where the generator creates fake data while the discriminator tries to distinguish real from fake.</p>

    <div class="callout info">
      <div class="callout-title">ðŸŽ¯ GAN History & Challenges (2014-2021)</div>
      <p><strong>GANs pioneered adversarial training but faced significant stability challenges before architectural innovations.</strong></p>
      <ul>
        <li><strong>Original Formulation (June 2014):</strong> Goodfellow et al. introduced min-max objective where generator G tries to fool discriminator D while D tries to distinguish real from fake: min_G max_D E[log D(x)] + E[log(1-D(G(z)))]. Zero-sum competition drives both networks to improve iteratively.</li>
        <li><strong>Training Challenges:</strong> <em>Unstable convergence</em> from searching for Nash equilibrium in high-dimensional space â†’ <em>Mode collapse</em> where generator produces limited variety ignoring parts of data distribution â†’ <em>Vanishing gradient</em> when discriminator becomes too good, generator receives no useful signal â†’ <em>Two Time-Scale Update Rule (TTUR)</em> helped stabilize by updating discriminator and generator at different rates.</li>
        <li><strong>DCGAN (2015):</strong> Radford et al. made GANs practical using only convolutional layers (no fully connected), batch normalization, LeakyReLU, and specific architectural guidelines making training significantly more stable.</li>
        <li><strong>WGAN (2016):</strong> Arjovsky et al. solved mode collapse by replacing Jensen-Shannon divergence with Wasserstein distance, enforcing 1-Lipschitz constraint via weight clipping (later improved with gradient penalty in WGAN-GP).</li>
        <li><strong>Progressive GAN (2017):</strong> Karras et al. (Nvidia) gradually increased resolution from 4Ã—4â†’8Ã—8â†’16Ã—16â†’...â†’1024Ã—1024 with layer blending, enabling high-resolution face generation for first time.</li>
        <li><strong>StyleGAN Series (2019-2021):</strong> StyleGAN-1 (2019) introduced style latent space mapping network zâ†’w and Adaptive Instance Normalization (AdaIN) controlling image style at multiple scales â†’ StyleGAN-2 (2020) improved by transforming convolution weights solving "blob problem" â†’ StyleGAN-2-ADA used adaptive data augmentation â†’ StyleGAN-3 (2021) solved texture sticking to pixel coordinates by applying Nyquist-Shannon sampling theorem with lowpass filters between layers for faithful continuous signal representation.</li>
        <li><strong>Notable Applications:</strong> October 2018 "Edmond de Belamy" portrait (trained on 15K WikiArt paintings 14th-19th century) sold for $432,500 at auction â†’ Video game texture upscaling to 4K â†’ May 2020 Nvidia GameGAN recreated Pac-Man by watching gameplay â†’ 2020 Artbreeder creative tool â†’ Face aging, 3D reconstruction, artistic style transfer.</li>
        <li><strong>Other Variants:</strong> CycleGAN (2017, Zhu et al.) unpaired image-to-image translation with cycle consistency â†’ SAGAN (2019, Zhang et al.) self-attention mechanism â†’ Conditional GANs controlling generation with class labels.</li>
      </ul>
      <p><em>Source: <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network" target="_blank">Wikipedia - Generative Adversarial Network</a></em></p>
    </div>

    <div class="diagram-container">
      <pre class="mermaid" set:html={`graph LR
    A[Random Noise z] --> B[Generator G]
    B --> C[Fake Image]
    D[Real Images] --> E[Discriminator D]
    C --> E
    E --> F[Real/Fake Classification]
    F --> G[Loss]
    G --> B
    G --> E
    
    style B fill:#3b82f6
    style E fill:#ec4899
      `} />
    </div>

    <h3>GAN Training Objective</h3>
    <pre><code>{`min_G max_D V(D,G) = E_x[log D(x)] + E_z[log(1 - D(G(z)))]

Generator: Minimize discriminator's ability to detect fakes
Discriminator: Maximize ability to distinguish real from fake`}
</code></pre>

    <h3>Popular GAN Architectures</h3>
    <table>
      <thead>
        <tr>
          <th>Architecture</th>
          <th>Year</th>
          <th>Innovation</th>
          <th>Use Case</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><strong>DCGAN</strong></td>
          <td>2015</td>
          <td>Convolutional architecture</td>
          <td>Image generation</td>
        </tr>
        <tr>
          <td><strong>StyleGAN</strong></td>
          <td>2019</td>
          <td>Style-based generator</td>
          <td>High-quality faces</td>
        </tr>
        <tr>
          <td><strong>CycleGAN</strong></td>
          <td>2017</td>
          <td>Unpaired image translation</td>
          <td>Style transfer</td>
        </tr>
        <tr>
          <td><strong>Pix2Pix</strong></td>
          <td>2017</td>
          <td>Conditional image-to-image</td>
          <td>Image translation</td>
        </tr>
        <tr>
          <td><strong>ProGAN</strong></td>
          <td>2018</td>
          <td>Progressive growing</td>
          <td>High-resolution images</td>
        </tr>
      </tbody>
    </table>

    <h3>GAN Training Challenges</h3>
    <ul>
      <li><strong>Mode Collapse:</strong> Generator produces limited variety</li>
      <li><strong>Training Instability:</strong> Difficult to balance G and D</li>
      <li><strong>Vanishing Gradients:</strong> When D becomes too strong</li>
      <li><strong>No Explicit Likelihood:</strong> Hard to evaluate quality objectively</li>
    </ul>

    <h2>Variational Autoencoders (VAEs)</h2>

    <p>VAEs learn to encode data into a latent space and decode back, with probabilistic constraints.</p>

    <div class="diagram-container">
      <pre class="mermaid" set:html={`graph LR
    A[Input Image x] --> B[Encoder q]
    B --> C[Latent z ~ N(Î¼,Ïƒ)]
    C --> D[Decoder p]
    D --> E[Reconstructed x']
    C --> F[KL Divergence]
    E --> G[Reconstruction Loss]
    F --> H[Total Loss]
    G --> H
    
    style B fill:#3b82f6
    style C fill:#8b5cf6
    style D fill:#ec4899
      `} />
    </div>

    <h3>VAE Loss Function</h3>
    <pre><code>{`L = Reconstruction Loss + KL Divergence

L = E[log p(x|z)] - KL[q(z|x) || p(z)]

Reconstruction: How well decoder recreates input
KL Divergence: Regularizes latent space to N(0,1)`}
</code></pre>

    <h3>VAE vs GAN</h3>
    <table>
      <thead>
        <tr>
          <th>Aspect</th>
          <th>VAE</th>
          <th>GAN</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><strong>Training</strong></td>
          <td>Stable, straightforward</td>
          <td>Unstable, adversarial</td>
        </tr>
        <tr>
          <td><strong>Output Quality</strong></td>
          <td>Blurry, smooth</td>
          <td>Sharp, realistic</td>
        </tr>
        <tr>
          <td><strong>Latent Space</strong></td>
          <td>Structured, continuous</td>
          <td>Less structured</td>
        </tr>
        <tr>
          <td><strong>Likelihood</strong></td>
          <td>Explicit (lower bound)</td>
          <td>Implicit</td>
        </tr>
        <tr>
          <td><strong>Use Case</strong></td>
          <td>Compression, interpolation</td>
          <td>High-quality synthesis</td>
        </tr>
      </tbody>
    </table>

    <h2>Diffusion Models</h2>

    <p>Diffusion models generate data by learning to reverse a gradual noising process. <strong>Introduced in 2015 by Sohl-Dickstein et al.</strong> using concepts from non-equilibrium thermodynamics, diffusion models have become the dominant approach for high-quality image generation by 2024, powering DALL-E 2, Stable Diffusion, Imagen, and Midjourney.</p>

    <div class="callout info">
      <div class="callout-title">ðŸŒŠ Diffusion Model Evolution (2015-2024)</div>
      <p><strong>Diffusion models evolved from slow 1000-step processes to fast transformer-based architectures dominating generative AI.</strong></p>
      <ul>
        <li><strong>Foundational Concept (2015):</strong> Sohl-Dickstein et al. introduced diffusion probabilistic models using non-equilibrium thermodynamics â†’ Forward process gradually adds Gaussian noise over T steps until image becomes pure noise â†’ Reverse denoising process learns to remove noise iteratively recovering original data distribution â†’ Trained using variational inference minimizing KL divergence.</li>
        <li><strong>DDPM Breakthrough (June 2020):</strong> Ho et al. "Denoising Diffusion Probabilistic Models" made diffusion practical with simplified training objective predicting noise at each step â†’ Required ~1000 timesteps for high quality making generation slow (minutes per image) â†’ U-Net backbone (originally for medical image segmentation) adapted for denoising with attention layers.</li>
        <li><strong>DDIM Fast Sampling (October 2020):</strong> Song et al. "Denoising Diffusion Implicit Models" allowed non-Markovian process enabling skipping steps â†’ Reduced sampling from 1000â†’50-100 steps achieving 10-50Ã— speedup while maintaining quality â†’ Deterministic sampling enabled image interpolation and editing.</li>
        <li><strong>Conditional Generation (2021):</strong> Dhariwal & Nichol introduced classifier guidance using pretrained classifier gradients âˆ‡_x log p(y|x) to steer generation toward desired class â†’ CLIP text encoders enabled text-to-image by conditioning on text embeddings â†’ Opened path to DALL-E 2 and Stable Diffusion.</li>
        <li><strong>Classifier-Free Guidance (July 2022):</strong> Ho & Salimans eliminated separate classifier by training single conditional model â†’ Mixed conditional and unconditional predictions weighted by guidance scale w typically 7-15 â†’ Balanced quality vs diversity, became standard in Stable Diffusion/DALL-E.</li>
        <li><strong>Latent Diffusion (April 2022):</strong> Rombach et al. "High-Resolution Image Synthesis with Latent Diffusion Models" operated in compressed latent space of pretrained VAE rather than pixel space â†’ Reduced computational cost 4-8Ã— enabling 512Ã—512 and 1024Ã—1024 generation on consumer GPUs â†’ Foundation of Stable Diffusion's accessibility.</li>
        <li><strong>Major Implementations:</strong> OpenAI: DALL-E original 2021 (actually autoregressive Transformer, not diffusion), GLIDE March 2022 (3.5B diffusion), DALL-E 2 April 2022 (3.5B cascaded diffusion with unCLIP inverting CLIP encoder: textâ†’CLIP text embeddingâ†’prior modelâ†’CLIP image embeddingâ†’decoderâ†’pixels), Sora February 2024 (DiT for video) â†’ Stability AI: Stable Diffusion August 2022 (860M latent diffusion with U-Net, open-source democratizing access), SD 3 March 2024 (DiT + rectified flow) â†’ Google: Imagen 2022 (T5-XXL encoder, cascaded: 2B 64Ã—64 base + 650M 256Ã—256 + 400M 1024Ã—1024), Veo 2024 (video) â†’ Meta: Make-A-Video 2022, Movie Gen October 2024 (DiT series).</li>
        <li><strong>Architectural Evolution (2023-2024):</strong> Diffusion Transformers (DiT) replacing U-Net backbones with transformer architecture â†’ Rectified Flow (Liu et al. September 2022) learning straight-line trajectories in probability space for faster sampling â†’ Consistency Models (Song et al. July 2023) enabling 1-step generation â†’ Progressive distillation iteratively reducing sampling steps â†’ Noise schedulers evolved from linear to cosine and custom designs.</li>
        <li><strong>Applications Beyond Images:</strong> Computer vision (inpainting, super-resolution, image-to-image translation, video generation) â†’ NLP (discrete diffusion for text generation, summarization) â†’ Audio synthesis (speech, music, sound effects) â†’ Reinforcement learning (planning, policy learning) â†’ 3D generation (novel view synthesis, 3D objects, motion).</li>
      </ul>
      <p><em>Source: <a href="https://en.wikipedia.org/wiki/Diffusion_model" target="_blank">Wikipedia - Diffusion Model</a></em></p>
    </div>

    <h3>Forward Process (Noising)</h3>
    <pre><code>{`q(x_t | x_{t-1}) = N(x_t; âˆš(1-Î²_t)Â·x_{t-1}, Î²_tÂ·I)

Start with real image x_0
Gradually add Gaussian noise over T steps
End with pure noise x_T ~ N(0,I)`}
</code></pre>

    <h3>Reverse Process (Denoising)</h3>
    <pre><code>{`p_Î¸(x_{t-1} | x_t) = N(x_{t-1}; Î¼_Î¸(x_t, t), Î£_Î¸(x_t, t))

Learn to predict noise at each step
Iteratively denoise from x_T to x_0
Generate new samples by sampling x_T and denoising`}
</code></pre>

    <div class="diagram-container">
      <pre class="mermaid" set:html={`graph LR
    A[Real Image x_0] --> B[+ Noise]
    B --> C[x_1]
    C --> D[+ Noise]
    D --> E[x_T Pure Noise]
    E --> F[- Noise]
    F --> G[x_{T-1}]
    G --> H[- Noise]
    H --> I[x_0 Generated]
    
    style A fill:#10b981
    style E fill:#ef4444
    style I fill:#3b82f6
      `} />
    </div>

    <h3>Diffusion Model Variants</h3>
    <table>
      <thead>
        <tr>
          <th>Model</th>
          <th>Key Feature</th>
          <th>Speed</th>
          <th>Year</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><strong>DDPM</strong></td>
          <td>Original diffusion, 1000 steps (Ho et al.)</td>
          <td>Slow (~minutes)</td>
          <td>June 2020</td>
        </tr>
        <tr>
          <td><strong>DDIM</strong></td>
          <td>Deterministic sampling, 50 steps (Song et al.)</td>
          <td>Fast (10-50Ã— faster)</td>
          <td>October 2020</td>
        </tr>
        <tr>
          <td><strong>Latent Diffusion</strong></td>
          <td>Diffusion in VAE latent space (Rombach et al.)</td>
          <td>Very fast (4-8Ã— savings)</td>
          <td>April 2022</td>
        </tr>
        <tr>
          <td><strong>Consistency Models</strong></td>
          <td>1-step generation (Song et al.)</td>
          <td>Ultra fast (~seconds)</td>
          <td>July 2023</td>
        </tr>
        <tr>
          <td><strong>Rectified Flow</strong></td>
          <td>Straight-line trajectories (Liu et al.)</td>
          <td>Very fast (fewer steps)</td>
          <td>September 2022</td>
        </tr>
      </tbody>
    </table>

    <h2>Text-to-Image Generation</h2>

    <h3>Stable Diffusion Architecture</h3>
    <div class="diagram-container">
      <pre class="mermaid" set:html={`graph TB
    A[Text Prompt] --> B[CLIP Text Encoder]
    B --> C[Text Embeddings]
    D[Random Noise] --> E[U-Net]
    C --> E
    E --> F[Denoised Latent]
    F --> G[VAE Decoder]
    G --> H[Generated Image]
    
    style B fill:#3b82f6
    style E fill:#8b5cf6
    style G fill:#ec4899
      `} />
    </div>

    <h3>Key Components</h3>
    <ul>
      <li><strong>Text Encoder (CLIP):</strong> Converts text to embeddings</li>
      <li><strong>U-Net:</strong> Predicts noise at each diffusion step</li>
      <li><strong>VAE:</strong> Compresses/decompresses images to/from latent space</li>
      <li><strong>Scheduler:</strong> Controls denoising schedule (Î² values)</li>
    </ul>

    <h3>Popular Text-to-Image Models</h3>
    <table>
      <thead>
        <tr>
          <th>Model</th>
          <th>Company</th>
          <th>Architecture</th>
          <th>Launch</th>
          <th>Strengths</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><strong>DALL-E 2</strong></td>
          <td>OpenAI</td>
          <td>Cascaded Diffusion (3.5B, unCLIP)</td>
          <td>April 2022</td>
          <td>High quality, CLIP alignment</td>
        </tr>
        <tr>
          <td><strong>DALL-E 3</strong></td>
          <td>OpenAI</td>
          <td>Diffusion (improved prompting)</td>
          <td>October 2023</td>
          <td>Prompt following, text rendering</td>
        </tr>
        <tr>
          <td><strong>Stable Diffusion</strong></td>
          <td>Stability AI</td>
          <td>Latent Diffusion (860M, U-Net)</td>
          <td>August 2022</td>
          <td>Open source, fast, customizable</td>
        </tr>
        <tr>
          <td><strong>Stable Diffusion 3</strong></td>
          <td>Stability AI</td>
          <td>DiT + Rectified Flow</td>
          <td>March 2024</td>
          <td>Improved quality, faster sampling</td>
        </tr>
        <tr>
          <td><strong>Midjourney V6</strong></td>
          <td>Midjourney</td>
          <td>Proprietary (likely diffusion)</td>
          <td>December 2023</td>
          <td>Artistic style, aesthetics, consistency</td>
        </tr>
        <tr>
          <td><strong>Imagen 2</strong></td>
          <td>Google</td>
          <td>Cascaded Diffusion (T5-XXL)</td>
          <td>December 2023</td>
          <td>Photorealism, text+image prompts</td>
        </tr>
        <tr>
          <td><strong>Firefly</strong></td>
          <td>Adobe</td>
          <td>Diffusion (commercially trained)</td>
          <td>March 2023</td>
          <td>Commercial safety, Adobe integration</td>
        </tr>
      </tbody>
    </table>

    <div class="callout info">
      <div class="callout-title">ðŸš€ Text-to-Image Timeline (2021-2024)</div>
      <ul>
        <li><strong>2021:</strong> DALL-E original (12B autoregressive Transformer, not diffusion) â†’ 2021 CLIP enables text-image alignment</li>
        <li><strong>March 2022:</strong> GLIDE (OpenAI 3.5B diffusion with classifier guidance)</li>
        <li><strong>April 2022:</strong> DALL-E 2 (3.5B cascaded diffusion with unCLIP technique) â†’ Latent Diffusion paper (Rombach et al.) foundation of Stable Diffusion</li>
        <li><strong>May 2022:</strong> Midjourney V1 launched (Discord-based, artistic focus)</li>
        <li><strong>August 2022:</strong> Stable Diffusion 1.4 released open-source (860M latent diffusion) democratizing text-to-image AI</li>
        <li><strong>2022:</strong> Imagen (Google T5-XXL + cascaded diffusion: 2B+650M+400M)</li>
        <li><strong>March 2023:</strong> Adobe Firefly (commercially safe training data), Midjourney V5</li>
        <li><strong>October 2023:</strong> DALL-E 3 (improved prompt following, integrated with ChatGPT)</li>
        <li><strong>December 2023:</strong> Midjourney V6 (enhanced photorealism), Imagen 2 (text+image prompts)</li>
        <li><strong>March 2024:</strong> Stable Diffusion 3 (switched to DiT architecture with rectified flow for faster straight-line sampling)</li>
        <li><strong>May 2024:</strong> Imagen 3 â†’ <strong>2024:</strong> Diffusion Transformers (DiT) increasingly replace U-Net backbones across industry</li>
      </ul>
    </div>

    <h3>Prompt Engineering for Image Generation</h3>
    <pre><code>{`Basic Prompt:
"A cat sitting on a chair"

Enhanced Prompt:
"A fluffy orange tabby cat sitting on a vintage wooden chair, 
soft natural lighting, bokeh background, detailed fur texture, 
professional photography, 4K, trending on artstation"

Key Elements:
- Subject: What to generate
- Style: Art style, photography type
- Lighting: Natural, studio, dramatic
- Quality: 4K, detailed, high resolution
- Modifiers: Trending on artstation, by artist name`}
</code></pre>

    <h2>Advanced Techniques</h2>

    <h3>ControlNet</h3>
    <p>Adds spatial conditioning to diffusion models for precise control.</p>
    <ul>
      <li><strong>Edge Detection:</strong> Generate from Canny edges</li>
      <li><strong>Pose Control:</strong> Use OpenPose skeleton</li>
      <li><strong>Depth Maps:</strong> Control spatial layout</li>
      <li><strong>Segmentation:</strong> Semantic region control</li>
    </ul>

    <h3>LoRA (Low-Rank Adaptation)</h3>
    <pre><code>{`Fine-tune diffusion models with minimal parameters:

Original weights: W (512Ã—512 = 262K params)
LoRA: W + Î”W = W + BÂ·A
      where A (512Ã—8), B (8Ã—512) = only 8K params!

Benefits:
- Fast training (minutes on consumer GPU)
- Small file size (~5-50 MB)
- Stackable (use multiple LoRAs together)
- Preserves base model quality`}
</code></pre>

    <h3>Inpainting & Outpainting</h3>
    <table>
      <thead>
        <tr>
          <th>Technique</th>
          <th>Description</th>
          <th>Use Case</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><strong>Inpainting</strong></td>
          <td>Fill masked regions</td>
          <td>Remove objects, edit parts</td>
        </tr>
        <tr>
          <td><strong>Outpainting</strong></td>
          <td>Extend image beyond borders</td>
          <td>Expand canvas, create panoramas</td>
        </tr>
        <tr>
          <td><strong>Image-to-Image</strong></td>
          <td>Transform existing image</td>
          <td>Style transfer, variations</td>
        </tr>
        <tr>
          <td><strong>Upscaling</strong></td>
          <td>Increase resolution</td>
          <td>Enhance details, enlarge</td>
        </tr>
      </tbody>
    </table>

    <h2>Evaluation Metrics</h2>

    <h3>Quantitative Metrics</h3>
    <table>
      <thead>
        <tr>
          <th>Metric</th>
          <th>Measures</th>
          <th>Range</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><strong>FID</strong></td>
          <td>FrÃ©chet Inception Distance</td>
          <td>Lower = better</td>
        </tr>
        <tr>
          <td><strong>IS</strong></td>
          <td>Inception Score</td>
          <td>Higher = better</td>
        </tr>
        <tr>
          <td><strong>CLIP Score</strong></td>
          <td>Text-image alignment</td>
          <td>Higher = better</td>
        </tr>
        <tr>
          <td><strong>LPIPS</strong></td>
          <td>Perceptual similarity</td>
          <td>Lower = more similar</td>
        </tr>
      </tbody>
    </table>

    <h3>Qualitative Evaluation</h3>
    <ul>
      <li><strong>Visual Quality:</strong> Sharpness, realism, artifacts</li>
      <li><strong>Prompt Adherence:</strong> Matches text description</li>
      <li><strong>Diversity:</strong> Variety in generated samples</li>
      <li><strong>Consistency:</strong> Coherence across generations</li>
    </ul>

    <div class="callout best-practice">
      <div class="callout-title">âœ… Best Practices</div>
      <ul>
        <li>Start with pre-trained models (Stable Diffusion, DALL-E)</li>
        <li>Use negative prompts to exclude unwanted elements</li>
        <li>Experiment with CFG scale (7-15 for balanced results)</li>
        <li>Fine-tune with LoRA for specific styles or subjects</li>
        <li>Batch generate multiple samples to select best output</li>
      </ul>
    </div>

    <h2>Ethical Considerations</h2>

    <h3>Key Concerns</h3>
    <ul>
      <li><strong>Copyright:</strong> Training on copyrighted images</li>
      <li><strong>Deepfakes:</strong> Generating realistic fake content</li>
      <li><strong>Bias:</strong> Reproducing stereotypes from training data</li>
      <li><strong>Attribution:</strong> Crediting artists whose style is mimicked</li>
      <li><strong>Misuse:</strong> NSFW content, misinformation</li>
    </ul>

    <h3>Safety Measures</h3>
    <table>
      <thead>
        <tr>
          <th>Measure</th>
          <th>Implementation</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><strong>Content Filtering</strong></td>
          <td>NSFW detectors, blocked prompts</td>
        </tr>
        <tr>
          <td><strong>Watermarking</strong></td>
          <td>Invisible markers to identify AI-generated</td>
        </tr>
        <tr>
          <td><strong>Model Cards</strong></td>
          <td>Document training data, limitations</td>
        </tr>
        <tr>
          <td><strong>Usage Policies</strong></td>
          <td>Terms of service, acceptable use</td>
        </tr>
      </tbody>
    </table>

    <h2>Summary</h2>

    <div class="callout info">
      <ul>
        <li>Generative AI creates new content by learning data distributions</li>
        <li>GANs use adversarial training but can be unstable</li>
        <li>VAEs provide structured latent spaces with stable training</li>
        <li>Diffusion models are current state-of-the-art for image generation</li>
        <li>Text-to-image models combine diffusion with language understanding</li>
        <li>ControlNet and LoRA enable fine-grained control</li>
        <li>Ethical considerations around copyright and misuse are critical</li>
      </ul>
    </div>

    <h2>Review Questions</h2>
    <ol>
      <li>Explain the difference between the generator and discriminator in a GAN. How do they interact during training?</li>
      <li>What is mode collapse in GANs and how can it be addressed?</li>
      <li>How does the loss function of a VAE balance reconstruction and regularization?</li>
      <li>Describe the forward and reverse processes in diffusion models.</li>
      <li>What are the key advantages of latent diffusion (Stable Diffusion) over pixel-space diffusion?</li>
      <li>How does ControlNet provide additional control over image generation?</li>
    </ol>

    <h2>Practical Exercises</h2>
    <ol>
      <li>
        <strong>Generate Images with Stable Diffusion</strong>
        <ul>
          <li>Install Stable Diffusion (Automatic1111 or ComfyUI)</li>
          <li>Experiment with different prompts and parameters</li>
          <li>Compare results with different CFG scales and steps</li>
        </ul>
      </li>
      <li>
        <strong>Fine-tune a LoRA</strong>
        <ul>
          <li>Collect 15-30 images of a specific subject or style</li>
          <li>Train a LoRA using Kohya or similar tool</li>
          <li>Test the LoRA with various prompts</li>
        </ul>
      </li>
      <li>
        <strong>Implement Image Evaluation</strong>
        <ul>
          <li>Generate 100 images with different prompts</li>
          <li>Calculate FID and CLIP scores</li>
          <li>Analyze correlation with human preferences</li>
        </ul>
      </li>
    </ol>

  </article>
</BaseLayout>
